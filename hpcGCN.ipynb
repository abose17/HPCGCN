{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 850,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "from collections import OrderedDict\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import trange, tqdm\n",
    "import time\n",
    "import metis\n",
    "import pickle\n",
    "from datetime import datetime, timedelta\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import torch\n",
    "import json\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "from scipy.stats import norm\n",
    "# torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 500\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slurm_Cleaned = pd.read_csv(\"/home/abose//HPC Analytics GCN/slurmCleaned.csv\")\n",
    "# slurm_userSurvey = pd.read_csv(\"/home/abose//HPC Analytics GCN/slurmUserSurvey.csv\")\n",
    "slurm_user_ladap_merged = pd.read_csv(\"/home/abose/HPC Analytics GCN/2020_21.csv\", low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6934624, 112)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slurm_user_ladap_merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1368,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>JobID</th>\n",
       "      <th>TimelimitRaw</th>\n",
       "      <th>ReqMem</th>\n",
       "      <th>NCPUS</th>\n",
       "      <th>NNodes</th>\n",
       "      <th>AveVMSize</th>\n",
       "      <th>AveRSS</th>\n",
       "      <th>MaxVMSize</th>\n",
       "      <th>MaxRSS</th>\n",
       "      <th>AssocID</th>\n",
       "      <th>ReqCPUS</th>\n",
       "      <th>NodeList</th>\n",
       "      <th>UID</th>\n",
       "      <th>role</th>\n",
       "      <th>GID</th>\n",
       "      <th>q5</th>\n",
       "      <th>q6</th>\n",
       "      <th>q7</th>\n",
       "      <th>AvePages</th>\n",
       "      <th>CPUTimeRAW</th>\n",
       "      <th>State</th>\n",
       "      <th>department</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8435065</td>\n",
       "      <td>34560.0</td>\n",
       "      <td>1Gc</td>\n",
       "      <td>32</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>625</td>\n",
       "      <td>32</td>\n",
       "      <td>dwarf36</td>\n",
       "      <td>2649.0</td>\n",
       "      <td>Faculty</td>\n",
       "      <td>2649.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64116096</td>\n",
       "      <td>FAILED</td>\n",
       "      <td>Chemistry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8435065.batch</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1Gc</td>\n",
       "      <td>32</td>\n",
       "      <td>1.0</td>\n",
       "      <td>173020K</td>\n",
       "      <td>8106244K</td>\n",
       "      <td>173020K</td>\n",
       "      <td>8106244K</td>\n",
       "      <td>625</td>\n",
       "      <td>32</td>\n",
       "      <td>dwarf36</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>131</td>\n",
       "      <td>64116096</td>\n",
       "      <td>FAILED</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8435065.extern</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1Gc</td>\n",
       "      <td>32</td>\n",
       "      <td>1.0</td>\n",
       "      <td>107952K</td>\n",
       "      <td>0</td>\n",
       "      <td>107952K</td>\n",
       "      <td>0</td>\n",
       "      <td>625</td>\n",
       "      <td>32</td>\n",
       "      <td>dwarf36</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>64116096</td>\n",
       "      <td>COMPLETED</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8389362</td>\n",
       "      <td>34560.0</td>\n",
       "      <td>1Gc</td>\n",
       "      <td>32</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>625</td>\n",
       "      <td>32</td>\n",
       "      <td>dwarf50</td>\n",
       "      <td>2649.0</td>\n",
       "      <td>Faculty</td>\n",
       "      <td>2649.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>66355360</td>\n",
       "      <td>TIMEOUT</td>\n",
       "      <td>Chemistry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8389362.batch</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1Gc</td>\n",
       "      <td>32</td>\n",
       "      <td>1.0</td>\n",
       "      <td>173032K</td>\n",
       "      <td>7099776K</td>\n",
       "      <td>173032K</td>\n",
       "      <td>7099776K</td>\n",
       "      <td>625</td>\n",
       "      <td>32</td>\n",
       "      <td>dwarf50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>66355424</td>\n",
       "      <td>CANCELLED</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8389362.extern</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1Gc</td>\n",
       "      <td>32</td>\n",
       "      <td>1.0</td>\n",
       "      <td>107952K</td>\n",
       "      <td>0</td>\n",
       "      <td>107952K</td>\n",
       "      <td>0</td>\n",
       "      <td>625</td>\n",
       "      <td>32</td>\n",
       "      <td>dwarf50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>66355360</td>\n",
       "      <td>COMPLETED</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8519238</td>\n",
       "      <td>34560.0</td>\n",
       "      <td>1Gc</td>\n",
       "      <td>32</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>625</td>\n",
       "      <td>32</td>\n",
       "      <td>dwarf37</td>\n",
       "      <td>2649.0</td>\n",
       "      <td>Faculty</td>\n",
       "      <td>2649.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29975232</td>\n",
       "      <td>FAILED</td>\n",
       "      <td>Chemistry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8519238.batch</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1Gc</td>\n",
       "      <td>32</td>\n",
       "      <td>1.0</td>\n",
       "      <td>173112K</td>\n",
       "      <td>5715592K</td>\n",
       "      <td>173112K</td>\n",
       "      <td>5715592K</td>\n",
       "      <td>625</td>\n",
       "      <td>32</td>\n",
       "      <td>dwarf37</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>152</td>\n",
       "      <td>29975232</td>\n",
       "      <td>FAILED</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8519238.extern</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1Gc</td>\n",
       "      <td>32</td>\n",
       "      <td>1.0</td>\n",
       "      <td>107952K</td>\n",
       "      <td>0</td>\n",
       "      <td>107952K</td>\n",
       "      <td>0</td>\n",
       "      <td>625</td>\n",
       "      <td>32</td>\n",
       "      <td>dwarf37</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>29975232</td>\n",
       "      <td>COMPLETED</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8532439</td>\n",
       "      <td>14400.0</td>\n",
       "      <td>150Gn</td>\n",
       "      <td>8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>651</td>\n",
       "      <td>8</td>\n",
       "      <td>hero45</td>\n",
       "      <td>2754.0</td>\n",
       "      <td>UndergraduateStudent</td>\n",
       "      <td>2754.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6912184</td>\n",
       "      <td>TIMEOUT</td>\n",
       "      <td>VeterinaryDiagnosticLaboratory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>8532439.batch</td>\n",
       "      <td>NaN</td>\n",
       "      <td>150Gn</td>\n",
       "      <td>8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>173048K</td>\n",
       "      <td>26793944K</td>\n",
       "      <td>173048K</td>\n",
       "      <td>26793944K</td>\n",
       "      <td>651</td>\n",
       "      <td>8</td>\n",
       "      <td>hero45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>45311</td>\n",
       "      <td>6912192</td>\n",
       "      <td>CANCELLED</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>8532439.extern</td>\n",
       "      <td>NaN</td>\n",
       "      <td>150Gn</td>\n",
       "      <td>8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>107952K</td>\n",
       "      <td>0</td>\n",
       "      <td>107952K</td>\n",
       "      <td>0</td>\n",
       "      <td>651</td>\n",
       "      <td>8</td>\n",
       "      <td>hero45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>6912184</td>\n",
       "      <td>COMPLETED</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             JobID  TimelimitRaw ReqMem  NCPUS  NNodes AveVMSize     AveRSS  \\\n",
       "0          8435065       34560.0    1Gc     32     1.0       NaN        NaN   \n",
       "1    8435065.batch           NaN    1Gc     32     1.0   173020K   8106244K   \n",
       "2   8435065.extern           NaN    1Gc     32     1.0   107952K          0   \n",
       "3          8389362       34560.0    1Gc     32     1.0       NaN        NaN   \n",
       "4    8389362.batch           NaN    1Gc     32     1.0   173032K   7099776K   \n",
       "5   8389362.extern           NaN    1Gc     32     1.0   107952K          0   \n",
       "6          8519238       34560.0    1Gc     32     1.0       NaN        NaN   \n",
       "7    8519238.batch           NaN    1Gc     32     1.0   173112K   5715592K   \n",
       "8   8519238.extern           NaN    1Gc     32     1.0   107952K          0   \n",
       "9          8532439       14400.0  150Gn      8     1.0       NaN        NaN   \n",
       "10   8532439.batch           NaN  150Gn      8     1.0   173048K  26793944K   \n",
       "11  8532439.extern           NaN  150Gn      8     1.0   107952K          0   \n",
       "\n",
       "   MaxVMSize     MaxRSS  AssocID  ReqCPUS NodeList     UID  \\\n",
       "0        NaN        NaN      625       32  dwarf36  2649.0   \n",
       "1    173020K   8106244K      625       32  dwarf36     NaN   \n",
       "2    107952K          0      625       32  dwarf36     NaN   \n",
       "3        NaN        NaN      625       32  dwarf50  2649.0   \n",
       "4    173032K   7099776K      625       32  dwarf50     NaN   \n",
       "5    107952K          0      625       32  dwarf50     NaN   \n",
       "6        NaN        NaN      625       32  dwarf37  2649.0   \n",
       "7    173112K   5715592K      625       32  dwarf37     NaN   \n",
       "8    107952K          0      625       32  dwarf37     NaN   \n",
       "9        NaN        NaN      651        8   hero45  2754.0   \n",
       "10   173048K  26793944K      651        8   hero45     NaN   \n",
       "11   107952K          0      651        8   hero45     NaN   \n",
       "\n",
       "                    role     GID   q5   q6   q7 AvePages  CPUTimeRAW  \\\n",
       "0                Faculty  2649.0  3.0  2.0  2.0      NaN    64116096   \n",
       "1                    NaN     NaN  NaN  NaN  NaN      131    64116096   \n",
       "2                    NaN     NaN  NaN  NaN  NaN        0    64116096   \n",
       "3                Faculty  2649.0  3.0  2.0  2.0      NaN    66355360   \n",
       "4                    NaN     NaN  NaN  NaN  NaN        0    66355424   \n",
       "5                    NaN     NaN  NaN  NaN  NaN        0    66355360   \n",
       "6                Faculty  2649.0  3.0  2.0  2.0      NaN    29975232   \n",
       "7                    NaN     NaN  NaN  NaN  NaN      152    29975232   \n",
       "8                    NaN     NaN  NaN  NaN  NaN        0    29975232   \n",
       "9   UndergraduateStudent  2754.0  4.0  4.0  3.0      NaN     6912184   \n",
       "10                   NaN     NaN  NaN  NaN  NaN    45311     6912192   \n",
       "11                   NaN     NaN  NaN  NaN  NaN        0     6912184   \n",
       "\n",
       "        State                      department  \n",
       "0      FAILED                       Chemistry  \n",
       "1      FAILED                             NaN  \n",
       "2   COMPLETED                             NaN  \n",
       "3     TIMEOUT                       Chemistry  \n",
       "4   CANCELLED                             NaN  \n",
       "5   COMPLETED                             NaN  \n",
       "6      FAILED                       Chemistry  \n",
       "7      FAILED                             NaN  \n",
       "8   COMPLETED                             NaN  \n",
       "9     TIMEOUT  VeterinaryDiagnosticLaboratory  \n",
       "10  CANCELLED                             NaN  \n",
       "11  COMPLETED                             NaN  "
      ]
     },
     "execution_count": 1368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dict(slurm_user_ladap_merged.head(1))\n",
    "features = ['JobID', 'TimelimitRaw', 'ReqMem', 'NCPUS', 'NNodes', 'AveVMSize', 'AveRSS', 'MaxVMSize', 'MaxRSS', 'AssocID', 'ReqCPUS', 'NodeList', 'UID', 'role', 'GID', 'q5', 'q6', 'q7', 'AvePages', 'CPUTimeRAW', 'State', 'department']\n",
    "slurm_Cleaned_Demo = slurm_user_ladap_merged[features].head(100000).copy()\n",
    "slurm_Cleaned_Demo.head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1369,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a\n",
       "0    14363\n",
       "1    85637\n",
       "dtype: int64"
      ]
     },
     "execution_count": 1369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slurm_Cleaned_Demo['State'] = slurm_Cleaned_Demo['State'].apply(lambda x: 1 if x == \"COMPLETED\" else 0)\n",
    "df = pd.DataFrame({'a':list(slurm_Cleaned_Demo['State'])})\n",
    "df.groupby('a').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1370,
   "metadata": {},
   "outputs": [],
   "source": [
    "slurm_Cleaned_Demo.drop(slurm_Cleaned_Demo[slurm_Cleaned_Demo['State'] == 0].index, inplace = True)  ##### Changed for Regression #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1371,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slurm_Cleaned_Demo.to_csv('/home/abose/HPC Analytics GCN/slurm_Cleaned_Demo.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1372,
   "metadata": {},
   "outputs": [],
   "source": [
    "slurm_Cleaned_Demo['MaxVMSize'] = slurm_Cleaned_Demo.MaxVMSize.str.extract('(\\d+)')\n",
    "slurm_Cleaned_Demo['MaxRSS'] = slurm_Cleaned_Demo.MaxRSS.str.extract('(\\d+)')\n",
    "slurm_Cleaned_Demo['AveVMSize'] = slurm_Cleaned_Demo.AveVMSize.str.extract('(\\d+)')\n",
    "slurm_Cleaned_Demo['AveRSS'] = slurm_Cleaned_Demo.AveRSS.str.extract('(\\d+)')\n",
    "slurm_Cleaned_Demo['AvePages'] = slurm_Cleaned_Demo.AveRSS.str.extract('(\\d+)')\n",
    "slurm_Cleaned_Demo['NewJobID'] = slurm_Cleaned_Demo['JobID'].apply(lambda x: str(x).split(\".\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1373,
   "metadata": {},
   "outputs": [],
   "source": [
    "slurm_Cleaned_Demo['NewJobID'] = slurm_Cleaned_Demo['JobID'].apply(lambda x: str(x).split(\".\")[0])\n",
    "slurm_Cleaned_Demo[['MaxVMSize', 'MaxRSS', 'AveVMSize', 'AveRSS', 'AssocID', 'ReqCPUS', 'AvePages']] = slurm_Cleaned_Demo[['MaxVMSize', 'MaxRSS', 'AveVMSize', 'AveRSS', 'AssocID', 'ReqCPUS', 'AvePages']].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JobID Input Processing ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1374,
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_join(list_names, concat='-'):\n",
    "    return concat.join(list_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1375,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_df = slurm_Cleaned_Demo.groupby('NewJobID')\n",
    "group_df = group_df.agg({'JobID':name_join, 'TimelimitRaw':'first', 'ReqMem':'first', 'NCPUS':'first', 'NNodes':'first', 'AveVMSize': 'max', 'AveRSS':'max', 'MaxVMSize': 'max', 'MaxRSS': 'max', 'AssocID': 'max', 'ReqCPUS':'max', 'NodeList':'first', 'UID':'first', 'role':'first' ,'GID':'first', 'q5':'max', 'q6':'max', 'q7':'max', 'AvePages':'max', 'CPUTimeRAW':'first', 'State':'first', 'department':'first'})\n",
    "s = group_df['JobID'].str.split('-').apply(pd.Series, 1).stack()\n",
    "s.index = s.index.droplevel(-1)\n",
    "s.name = 'JobID'\n",
    "del group_df['JobID']\n",
    "group_df = group_df.join(s)\n",
    "group_df = group_df.reset_index()\n",
    "group_df.drop_duplicates(subset = ['NewJobID'], keep = 'first', inplace = True)\n",
    "slurm_Cleaned_Demo = group_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1376,
   "metadata": {},
   "outputs": [],
   "source": [
    "### I SHOULD CHECK THE LIST CAREFULLY TO INCLUDE OR EXCLUDE SOME OF THE FEATURES\n",
    "\n",
    "# X_listFeatures = ['failed', 'State', 'JobIDRaw', 'Account','AssocID', 'AllocCPUS', 'CPUTimeRAW', 'NCPUS', 'NNodes']\n",
    "# X_listFeatures_1 = ['JobID', 'TimelimitRaw', 'ReqMem', 'NCPUS', 'NNodes', 'AveVMSize', 'AveRSS', 'MaxVMSize', 'MaxRSS', 'AssocID', 'ReqCPUS', 'NodeList', 'UID', 'role', 'GID', 'q5', 'q6', 'q7', 'AvePages', 'CPUTimeRAW', 'State', 'department']\n",
    "# X_listFeatures_1 = ['JobID', 'CPUTimeRAW', 'ReqMem', 'NCPUS', 'NNodes', 'AveVMSize', 'AveRSS', 'AssocID', 'ReqCPUS', 'NodeList', 'UID', 'role', 'GID', 'q5', 'q6', 'q7']\n",
    "# demoDF = slurm_Cleaned_Demo[X_listFeatures_1].dropna()\n",
    "# demoDF.to_csv('/home/abose/HPC Analytics GCN/demoDF.csv', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReqMem Numeric Parsing ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1377,
   "metadata": {},
   "outputs": [],
   "source": [
    "def memoryScaleConverter(Job):\n",
    "    if 'Gc' in Job['ReqMem'] or 'Gn'in Job['ReqMem']:\n",
    "        if 'Gc' in Job['ReqMem']:\n",
    "            req_Mem = float(\"\".join(list(re.compile(r\"(\\d+)Gc|(\\d+)Gn\").findall(Job['ReqMem'])[0])))\n",
    "            return req_Mem*Job['NCPUS']\n",
    "        if 'Gn'in Job['ReqMem']:\n",
    "            req_Mem = float(\"\".join(list(re.compile(r\"(\\d+)Gc|(\\d+)Gn\").findall(Job['ReqMem'])[0])))\n",
    "            return req_Mem*Job['NNodes']\n",
    "    if 'Mc' in Job['ReqMem'] or 'Mn'in Job['ReqMem']:\n",
    "        if 'Mc' in Job['ReqMem']:\n",
    "            req_Mem = float(\"\".join(list(re.compile(r\"(\\d+)Mn|(\\d+)Mc\").findall(Job['ReqMem'])[0])))/1024\n",
    "            return req_Mem*Job['NCPUS']\n",
    "        if 'Mn' in Job['ReqMem']:\n",
    "            req_Mem = float(\"\".join(list(re.compile(r\"(\\d+)Mn|(\\d+)Mc\").findall(Job['ReqMem'])[0])))/1024\n",
    "            return req_Mem*Job['NNodes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1378,
   "metadata": {},
   "outputs": [],
   "source": [
    "slurm_Cleaned_Demo['ReqMem'] = slurm_Cleaned_Demo.apply(lambda row: memoryScaleConverter(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1379,
   "metadata": {},
   "outputs": [],
   "source": [
    "slurm_Cleaned_Demo.dropna(subset=features,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1380,
   "metadata": {},
   "outputs": [],
   "source": [
    "slurm_Cleaned_Demo['JobID'] = slurm_Cleaned_Demo['NewJobID'].copy()\n",
    "del slurm_Cleaned_Demo['NewJobID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1381,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1381-dfa2c44603b8>:2: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "  average = slurm_Cleaned_Demo.groupby('UID', as_index=False)['CPUTimeRAW', 'MaxVMSize', 'TimelimitRaw', 'ReqMem', 'MaxRSS'].mean()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['TimelimitRaw', 'ReqMem', 'NCPUS', 'NNodes', 'AveVMSize', 'AveRSS',\n",
       "       'MaxVMSize', 'MaxRSS', 'AssocID', 'ReqCPUS', 'NodeList', 'UID', 'role',\n",
       "       'GID', 'q5', 'q6', 'q7', 'AvePages', 'State', 'department', 'JobID',\n",
       "       'aCPUTimeRAW', 'aMaxVMSize', 'aTimelimitRaw', 'aReqMem', 'aMaxRSS'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 1381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### Changed for Regression #####\n",
    "average = slurm_Cleaned_Demo.groupby('UID', as_index=False)['CPUTimeRAW', 'MaxVMSize', 'TimelimitRaw', 'ReqMem', 'MaxRSS'].mean()\n",
    "average.columns = ['UID','aCPUTimeRAW','aMaxVMSize','aTimelimitRaw','aReqMem','aMaxRSS']\n",
    "slurm_Cleaned_Demo = pd.merge(slurm_Cleaned_Demo, average, on=['UID'])\n",
    "# del slurm_Cleaned_Demo['MaxRSS'] #### Include Either This 'MaxRSS' ####\n",
    "del slurm_Cleaned_Demo['CPUTimeRAW'] #### Include Either This 'CPUTimeRAW' ####\n",
    "slurm_Cleaned_Demo.columns\n",
    "##### Changed for Regression #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1382,
   "metadata": {},
   "outputs": [],
   "source": [
    "slurm_Cleaned_Demo = pd.get_dummies(slurm_Cleaned_Demo, columns=['department', 'role'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1383,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape->  (22829, 43)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TimelimitRaw</th>\n",
       "      <th>ReqMem</th>\n",
       "      <th>NCPUS</th>\n",
       "      <th>NNodes</th>\n",
       "      <th>AveVMSize</th>\n",
       "      <th>AveRSS</th>\n",
       "      <th>MaxVMSize</th>\n",
       "      <th>MaxRSS</th>\n",
       "      <th>AssocID</th>\n",
       "      <th>ReqCPUS</th>\n",
       "      <th>NodeList</th>\n",
       "      <th>UID</th>\n",
       "      <th>GID</th>\n",
       "      <th>q5</th>\n",
       "      <th>q6</th>\n",
       "      <th>q7</th>\n",
       "      <th>AvePages</th>\n",
       "      <th>State</th>\n",
       "      <th>JobID</th>\n",
       "      <th>aCPUTimeRAW</th>\n",
       "      <th>aMaxVMSize</th>\n",
       "      <th>aTimelimitRaw</th>\n",
       "      <th>aReqMem</th>\n",
       "      <th>aMaxRSS</th>\n",
       "      <th>department_Agronomy</th>\n",
       "      <th>department_AnatomyandPhysiology</th>\n",
       "      <th>department_Biology</th>\n",
       "      <th>department_ChemicalEngineering</th>\n",
       "      <th>department_Chemistry</th>\n",
       "      <th>department_CivilEngineering</th>\n",
       "      <th>department_ComputerScience</th>\n",
       "      <th>department_InstituteforEnvironmentalResearch</th>\n",
       "      <th>department_MathematicsandStatistics</th>\n",
       "      <th>department_Mechanical&amp;NuclearEngineering</th>\n",
       "      <th>department_Physics</th>\n",
       "      <th>department_PlantPathology</th>\n",
       "      <th>department_VeterinaryDiagnosticLaboratory</th>\n",
       "      <th>role_Faculty</th>\n",
       "      <th>role_GraduateStudent</th>\n",
       "      <th>role_PostDoctoralResearcher</th>\n",
       "      <th>role_ResearchAssociate</th>\n",
       "      <th>role_Staff</th>\n",
       "      <th>role_UndergraduateStudent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10080.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100</td>\n",
       "      <td>8.0</td>\n",
       "      <td>447735515.0</td>\n",
       "      <td>4.902704e+09</td>\n",
       "      <td>438772.0</td>\n",
       "      <td>10970696.0</td>\n",
       "      <td>2080.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>elf[33-34,41-42,44,46,49,51]</td>\n",
       "      <td>3363.0</td>\n",
       "      <td>3363.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.902704e+09</td>\n",
       "      <td>1</td>\n",
       "      <td>8853689</td>\n",
       "      <td>4.123228e+07</td>\n",
       "      <td>438045.333333</td>\n",
       "      <td>11520.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.148973e+07</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10080.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100</td>\n",
       "      <td>13.0</td>\n",
       "      <td>446293674.0</td>\n",
       "      <td>9.046128e+06</td>\n",
       "      <td>437196.0</td>\n",
       "      <td>9598928.0</td>\n",
       "      <td>2080.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>elf[06-08,10,12-14,17,26,29,40,44],hero22</td>\n",
       "      <td>3363.0</td>\n",
       "      <td>3363.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.046128e+06</td>\n",
       "      <td>1</td>\n",
       "      <td>8863178</td>\n",
       "      <td>4.123228e+07</td>\n",
       "      <td>438045.333333</td>\n",
       "      <td>11520.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.148973e+07</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10080.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100</td>\n",
       "      <td>13.0</td>\n",
       "      <td>441298602.0</td>\n",
       "      <td>5.897932e+06</td>\n",
       "      <td>431084.0</td>\n",
       "      <td>7463672.0</td>\n",
       "      <td>2080.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>hero[18-19,22-27,29,31-33,53]</td>\n",
       "      <td>3363.0</td>\n",
       "      <td>3363.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.897932e+06</td>\n",
       "      <td>1</td>\n",
       "      <td>8863180</td>\n",
       "      <td>4.123228e+07</td>\n",
       "      <td>438045.333333</td>\n",
       "      <td>11520.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.148973e+07</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10080.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100</td>\n",
       "      <td>7.0</td>\n",
       "      <td>436630.0</td>\n",
       "      <td>1.007994e+07</td>\n",
       "      <td>438992.0</td>\n",
       "      <td>12299816.0</td>\n",
       "      <td>2080.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>elf[26,33-34,41-42,44,46]</td>\n",
       "      <td>3363.0</td>\n",
       "      <td>3363.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.007994e+07</td>\n",
       "      <td>1</td>\n",
       "      <td>8863181</td>\n",
       "      <td>4.123228e+07</td>\n",
       "      <td>438045.333333</td>\n",
       "      <td>11520.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.148973e+07</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14400.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100</td>\n",
       "      <td>16.0</td>\n",
       "      <td>445267148.0</td>\n",
       "      <td>4.098618e+09</td>\n",
       "      <td>440736.0</td>\n",
       "      <td>12173984.0</td>\n",
       "      <td>2080.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>elf[06-07,10,12,24,33,51,53],hero[17,20-21,24,...</td>\n",
       "      <td>3363.0</td>\n",
       "      <td>3363.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.098618e+09</td>\n",
       "      <td>1</td>\n",
       "      <td>8926397</td>\n",
       "      <td>4.123228e+07</td>\n",
       "      <td>438045.333333</td>\n",
       "      <td>11520.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.148973e+07</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22824</th>\n",
       "      <td>37440.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>173176.0</td>\n",
       "      <td>5.552184e+06</td>\n",
       "      <td>173176.0</td>\n",
       "      <td>5552184.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>mole036</td>\n",
       "      <td>2547.0</td>\n",
       "      <td>2548.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.552184e+06</td>\n",
       "      <td>1</td>\n",
       "      <td>8941283</td>\n",
       "      <td>9.464128e+06</td>\n",
       "      <td>173172.000000</td>\n",
       "      <td>34320.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>5.547957e+06</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22825</th>\n",
       "      <td>37440.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>173176.0</td>\n",
       "      <td>5.440292e+06</td>\n",
       "      <td>173176.0</td>\n",
       "      <td>5440292.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>mole037</td>\n",
       "      <td>2547.0</td>\n",
       "      <td>2548.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.440292e+06</td>\n",
       "      <td>1</td>\n",
       "      <td>8941286</td>\n",
       "      <td>9.464128e+06</td>\n",
       "      <td>173172.000000</td>\n",
       "      <td>34320.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>5.547957e+06</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22826</th>\n",
       "      <td>37440.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>173176.0</td>\n",
       "      <td>5.702324e+06</td>\n",
       "      <td>173176.0</td>\n",
       "      <td>5702324.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>mole040</td>\n",
       "      <td>2547.0</td>\n",
       "      <td>2548.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.702324e+06</td>\n",
       "      <td>1</td>\n",
       "      <td>8941452</td>\n",
       "      <td>9.464128e+06</td>\n",
       "      <td>173172.000000</td>\n",
       "      <td>34320.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>5.547957e+06</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22827</th>\n",
       "      <td>60.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>173120.0</td>\n",
       "      <td>5.519200e+04</td>\n",
       "      <td>173120.0</td>\n",
       "      <td>55192.0</td>\n",
       "      <td>477.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>wizard22</td>\n",
       "      <td>2315.0</td>\n",
       "      <td>2316.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.519200e+04</td>\n",
       "      <td>1</td>\n",
       "      <td>8938949</td>\n",
       "      <td>2.720000e+02</td>\n",
       "      <td>173120.000000</td>\n",
       "      <td>60.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.519200e+04</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22828</th>\n",
       "      <td>5760.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>173100.0</td>\n",
       "      <td>2.354282e+08</td>\n",
       "      <td>173100.0</td>\n",
       "      <td>235428192.0</td>\n",
       "      <td>2056.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>hero37</td>\n",
       "      <td>3339.0</td>\n",
       "      <td>3339.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.354282e+08</td>\n",
       "      <td>1</td>\n",
       "      <td>8942139</td>\n",
       "      <td>5.029920e+05</td>\n",
       "      <td>173100.000000</td>\n",
       "      <td>5760.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>2.354282e+08</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22829 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       TimelimitRaw  ReqMem  NCPUS  NNodes    AveVMSize        AveRSS  \\\n",
       "0           10080.0   100.0    100     8.0  447735515.0  4.902704e+09   \n",
       "1           10080.0   100.0    100    13.0  446293674.0  9.046128e+06   \n",
       "2           10080.0   100.0    100    13.0  441298602.0  5.897932e+06   \n",
       "3           10080.0   100.0    100     7.0     436630.0  1.007994e+07   \n",
       "4           14400.0   100.0    100    16.0  445267148.0  4.098618e+09   \n",
       "...             ...     ...    ...     ...          ...           ...   \n",
       "22824       37440.0    20.0     20     1.0     173176.0  5.552184e+06   \n",
       "22825       37440.0    20.0     20     1.0     173176.0  5.440292e+06   \n",
       "22826       37440.0    20.0     20     1.0     173176.0  5.702324e+06   \n",
       "22827          60.0     8.0      8     1.0     173120.0  5.519200e+04   \n",
       "22828        5760.0   240.0      4     1.0     173100.0  2.354282e+08   \n",
       "\n",
       "       MaxVMSize       MaxRSS  AssocID  ReqCPUS  \\\n",
       "0       438772.0   10970696.0   2080.0    100.0   \n",
       "1       437196.0    9598928.0   2080.0    100.0   \n",
       "2       431084.0    7463672.0   2080.0    100.0   \n",
       "3       438992.0   12299816.0   2080.0    100.0   \n",
       "4       440736.0   12173984.0   2080.0    100.0   \n",
       "...          ...          ...      ...      ...   \n",
       "22824   173176.0    5552184.0    250.0     20.0   \n",
       "22825   173176.0    5440292.0    250.0     20.0   \n",
       "22826   173176.0    5702324.0    250.0     20.0   \n",
       "22827   173120.0      55192.0    477.0      8.0   \n",
       "22828   173100.0  235428192.0   2056.0      4.0   \n",
       "\n",
       "                                                NodeList     UID     GID   q5  \\\n",
       "0                           elf[33-34,41-42,44,46,49,51]  3363.0  3363.0  2.0   \n",
       "1              elf[06-08,10,12-14,17,26,29,40,44],hero22  3363.0  3363.0  2.0   \n",
       "2                          hero[18-19,22-27,29,31-33,53]  3363.0  3363.0  2.0   \n",
       "3                              elf[26,33-34,41-42,44,46]  3363.0  3363.0  2.0   \n",
       "4      elf[06-07,10,12,24,33,51,53],hero[17,20-21,24,...  3363.0  3363.0  2.0   \n",
       "...                                                  ...     ...     ...  ...   \n",
       "22824                                            mole036  2547.0  2548.0  4.0   \n",
       "22825                                            mole037  2547.0  2548.0  4.0   \n",
       "22826                                            mole040  2547.0  2548.0  4.0   \n",
       "22827                                           wizard22  2315.0  2316.0  5.0   \n",
       "22828                                             hero37  3339.0  3339.0  2.0   \n",
       "\n",
       "        q6   q7      AvePages  State    JobID   aCPUTimeRAW     aMaxVMSize  \\\n",
       "0      3.0  1.0  4.902704e+09      1  8853689  4.123228e+07  438045.333333   \n",
       "1      3.0  1.0  9.046128e+06      1  8863178  4.123228e+07  438045.333333   \n",
       "2      3.0  1.0  5.897932e+06      1  8863180  4.123228e+07  438045.333333   \n",
       "3      3.0  1.0  1.007994e+07      1  8863181  4.123228e+07  438045.333333   \n",
       "4      3.0  1.0  4.098618e+09      1  8926397  4.123228e+07  438045.333333   \n",
       "...    ...  ...           ...    ...      ...           ...            ...   \n",
       "22824  3.0  1.0  5.552184e+06      1  8941283  9.464128e+06  173172.000000   \n",
       "22825  3.0  1.0  5.440292e+06      1  8941286  9.464128e+06  173172.000000   \n",
       "22826  3.0  1.0  5.702324e+06      1  8941452  9.464128e+06  173172.000000   \n",
       "22827  4.0  2.0  5.519200e+04      1  8938949  2.720000e+02  173120.000000   \n",
       "22828  2.0  1.0  2.354282e+08      1  8942139  5.029920e+05  173100.000000   \n",
       "\n",
       "       aTimelimitRaw  aReqMem       aMaxRSS  department_Agronomy  \\\n",
       "0            11520.0    100.0  1.148973e+07                    0   \n",
       "1            11520.0    100.0  1.148973e+07                    0   \n",
       "2            11520.0    100.0  1.148973e+07                    0   \n",
       "3            11520.0    100.0  1.148973e+07                    0   \n",
       "4            11520.0    100.0  1.148973e+07                    0   \n",
       "...              ...      ...           ...                  ...   \n",
       "22824        34320.0     20.0  5.547957e+06                    0   \n",
       "22825        34320.0     20.0  5.547957e+06                    0   \n",
       "22826        34320.0     20.0  5.547957e+06                    0   \n",
       "22827           60.0      8.0  5.519200e+04                    0   \n",
       "22828         5760.0    240.0  2.354282e+08                    1   \n",
       "\n",
       "       department_AnatomyandPhysiology  department_Biology  \\\n",
       "0                                    0                   0   \n",
       "1                                    0                   0   \n",
       "2                                    0                   0   \n",
       "3                                    0                   0   \n",
       "4                                    0                   0   \n",
       "...                                ...                 ...   \n",
       "22824                                0                   0   \n",
       "22825                                0                   0   \n",
       "22826                                0                   0   \n",
       "22827                                0                   1   \n",
       "22828                                0                   0   \n",
       "\n",
       "       department_ChemicalEngineering  department_Chemistry  \\\n",
       "0                                   0                     0   \n",
       "1                                   0                     0   \n",
       "2                                   0                     0   \n",
       "3                                   0                     0   \n",
       "4                                   0                     0   \n",
       "...                               ...                   ...   \n",
       "22824                               0                     1   \n",
       "22825                               0                     1   \n",
       "22826                               0                     1   \n",
       "22827                               0                     0   \n",
       "22828                               0                     0   \n",
       "\n",
       "       department_CivilEngineering  department_ComputerScience  \\\n",
       "0                                0                           0   \n",
       "1                                0                           0   \n",
       "2                                0                           0   \n",
       "3                                0                           0   \n",
       "4                                0                           0   \n",
       "...                            ...                         ...   \n",
       "22824                            0                           0   \n",
       "22825                            0                           0   \n",
       "22826                            0                           0   \n",
       "22827                            0                           0   \n",
       "22828                            0                           0   \n",
       "\n",
       "       department_InstituteforEnvironmentalResearch  \\\n",
       "0                                                 0   \n",
       "1                                                 0   \n",
       "2                                                 0   \n",
       "3                                                 0   \n",
       "4                                                 0   \n",
       "...                                             ...   \n",
       "22824                                             0   \n",
       "22825                                             0   \n",
       "22826                                             0   \n",
       "22827                                             0   \n",
       "22828                                             0   \n",
       "\n",
       "       department_MathematicsandStatistics  \\\n",
       "0                                        0   \n",
       "1                                        0   \n",
       "2                                        0   \n",
       "3                                        0   \n",
       "4                                        0   \n",
       "...                                    ...   \n",
       "22824                                    0   \n",
       "22825                                    0   \n",
       "22826                                    0   \n",
       "22827                                    0   \n",
       "22828                                    0   \n",
       "\n",
       "       department_Mechanical&NuclearEngineering  department_Physics  \\\n",
       "0                                             0                   1   \n",
       "1                                             0                   1   \n",
       "2                                             0                   1   \n",
       "3                                             0                   1   \n",
       "4                                             0                   1   \n",
       "...                                         ...                 ...   \n",
       "22824                                         0                   0   \n",
       "22825                                         0                   0   \n",
       "22826                                         0                   0   \n",
       "22827                                         0                   0   \n",
       "22828                                         0                   0   \n",
       "\n",
       "       department_PlantPathology  department_VeterinaryDiagnosticLaboratory  \\\n",
       "0                              0                                          0   \n",
       "1                              0                                          0   \n",
       "2                              0                                          0   \n",
       "3                              0                                          0   \n",
       "4                              0                                          0   \n",
       "...                          ...                                        ...   \n",
       "22824                          0                                          0   \n",
       "22825                          0                                          0   \n",
       "22826                          0                                          0   \n",
       "22827                          0                                          0   \n",
       "22828                          0                                          0   \n",
       "\n",
       "       role_Faculty  role_GraduateStudent  role_PostDoctoralResearcher  \\\n",
       "0                 0                     1                            0   \n",
       "1                 0                     1                            0   \n",
       "2                 0                     1                            0   \n",
       "3                 0                     1                            0   \n",
       "4                 0                     1                            0   \n",
       "...             ...                   ...                          ...   \n",
       "22824             0                     1                            0   \n",
       "22825             0                     1                            0   \n",
       "22826             0                     1                            0   \n",
       "22827             0                     0                            1   \n",
       "22828             0                     0                            0   \n",
       "\n",
       "       role_ResearchAssociate  role_Staff  role_UndergraduateStudent  \n",
       "0                           0           0                          0  \n",
       "1                           0           0                          0  \n",
       "2                           0           0                          0  \n",
       "3                           0           0                          0  \n",
       "4                           0           0                          0  \n",
       "...                       ...         ...                        ...  \n",
       "22824                       0           0                          0  \n",
       "22825                       0           0                          0  \n",
       "22826                       0           0                          0  \n",
       "22827                       0           0                          0  \n",
       "22828                       0           0                          1  \n",
       "\n",
       "[22829 rows x 43 columns]"
      ]
     },
     "execution_count": 1383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"shape-> \", slurm_Cleaned_Demo.shape)\n",
    "slurm_Cleaned_Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1384,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ['JobID', 'TimelimitRaw', 'ReqMem', 'NCPUS', 'NNodes', 'AveVMSize', 'AveRSS', 'MaxVMSize', 'MaxRSS', 'AssocID', 'ReqCPUS', 'NodeList', 'UID', 'role', 'GID', 'q5', 'q6', 'q7', 'AvePages', 'CPUTimeRAW', 'State', 'department']\n",
    "# list_neumeric = ['aCPUTimeRAW','aMaxVMSize','aTimelimitRaw','aReqMem', 'aMaxRSS', 'TimelimitRaw', 'ReqMem', 'NCPUS', 'NNodes', 'AveVMSize', 'AveRSS', 'MaxVMSize', 'ReqCPUS', 'q5', 'q6', 'q7', 'AvePages', 'department_Agronomy', 'department_ChemicalEngineering', 'department_Chemistry', 'department_ComputerScience', 'department_InstituteforEnvironmentalResearch', 'department_Mechanical&NuclearEngineering', 'department_Physics', 'department_PlantPathology', 'department_VeterinaryDiagnosticLaboratory', 'role_Faculty', 'role_GraduateStudent', 'role_PostDoctoralResearcher', 'role_ResearchAssociate', 'role_UndergraduateStudent'] ##### FOR Classification #####\n",
    "### , 'role_UndergraduateStudent', 'department_InstituteforEnvironmentalResearch', 'MaxRSS', 'CPUTimeRAW'\n",
    "list_neumeric = ['aCPUTimeRAW','aMaxVMSize','aTimelimitRaw','aReqMem', 'aMaxRSS', 'TimelimitRaw', 'ReqMem', 'NCPUS', 'NNodes', 'AveVMSize', 'AveRSS', 'MaxVMSize', 'ReqCPUS', 'q5', 'q6', 'q7', 'AvePages', 'department_Agronomy', 'department_ChemicalEngineering', 'department_Chemistry', 'department_ComputerScience', 'department_Mechanical&NuclearEngineering', 'department_Physics', 'department_PlantPathology', 'role_Faculty', 'role_GraduateStudent', 'role_PostDoctoralResearcher', 'role_ResearchAssociate']  ##### Changed for Regression #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1385,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_neumeric = ['TimelimitRaw', 'ReqMem', 'NCPUS', 'NNodes', 'AveVMSize', 'AveRSS', 'MaxVMSize', 'MaxRSS', 'ReqCPUS', 'q5', 'q6', 'q7', 'AvePages', 'CPUTimeRAW', 'department_AgriculturalEconomics', 'department_Agronomy', 'department_AnatomyandPhysiology', 'department_BiochemistryandMolecularBiophysics', 'department_BiologicalSciences', 'department_Biology', 'department_BiomedicalandHealthInformatics', 'department_ChemicalEngineering', 'department_Chemistry', 'department_CivilEngineering', 'department_ComputerScience', 'department_Economics', 'department_Entomology', 'department_Geography', 'department_InstituteforEnvironmentalResearch', 'department_MathematicsandStatistics', 'department_Mechanical&NuclearEngineering', 'department_Physics', 'department_PlantPathology', 'department_PsychologicalSciences', 'department_Statistics', 'department_VeterinaryDiagnosticLaboratory', 'role_Faculty', 'role_GraduateStudent', 'role_PostDoctoralResearcher', 'role_ResearchAssociate', 'role_Staff', 'role_UndergraduateStudent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1386,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_slurm_Cleaned_Demo=(slurm_Cleaned_Demo[list_neumeric]-slurm_Cleaned_Demo[list_neumeric].min())/(slurm_Cleaned_Demo[list_neumeric].max()-slurm_Cleaned_Demo[list_neumeric].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1387,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22829, 28)"
      ]
     },
     "execution_count": 1387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_slurm_Cleaned_Demo.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding JobID Column to the Normalized Neumeric Dataset ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1388,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_slurm_Cleaned_Demo['JobID'] = slurm_Cleaned_Demo['JobID'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1389,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_column = normalized_slurm_Cleaned_Demo.pop('JobID')\n",
    "normalized_slurm_Cleaned_Demo.insert(0, 'JobID', first_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1390,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_slurm_Cleaned_Demo.columns = range(normalized_slurm_Cleaned_Demo.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1391,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_slurm_Cleaned_dictionary = normalized_slurm_Cleaned_Demo.set_index(0).agg(list, axis=1).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1392,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_obj(obj, name ):\n",
    "    with open('obj/'+ name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "save_obj(normalized_slurm_Cleaned_dictionary, 'slurm_job_feature_dictionary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1393,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8853689</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.246736</td>\n",
       "      <td>0.284650</td>\n",
       "      <td>0.165275</td>\n",
       "      <td>0.048804</td>\n",
       "      <td>0.248882</td>\n",
       "      <td>0.066044</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.308225</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.247279</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8863178</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.246736</td>\n",
       "      <td>0.284650</td>\n",
       "      <td>0.165275</td>\n",
       "      <td>0.048804</td>\n",
       "      <td>0.248882</td>\n",
       "      <td>0.066044</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.307232</td>\n",
       "      <td>0.001845</td>\n",
       "      <td>0.246101</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001845</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8863180</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.246736</td>\n",
       "      <td>0.284650</td>\n",
       "      <td>0.165275</td>\n",
       "      <td>0.048804</td>\n",
       "      <td>0.248882</td>\n",
       "      <td>0.066044</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.303792</td>\n",
       "      <td>0.001203</td>\n",
       "      <td>0.241533</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001203</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8863181</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.246736</td>\n",
       "      <td>0.284650</td>\n",
       "      <td>0.165275</td>\n",
       "      <td>0.048804</td>\n",
       "      <td>0.248882</td>\n",
       "      <td>0.066044</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.000226</td>\n",
       "      <td>0.002056</td>\n",
       "      <td>0.247444</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002056</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8926397</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.246736</td>\n",
       "      <td>0.284650</td>\n",
       "      <td>0.165275</td>\n",
       "      <td>0.048804</td>\n",
       "      <td>0.356185</td>\n",
       "      <td>0.066044</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.306525</td>\n",
       "      <td>0.835991</td>\n",
       "      <td>0.248747</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.835991</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22824</th>\n",
       "      <td>8941283</td>\n",
       "      <td>0.229532</td>\n",
       "      <td>0.048750</td>\n",
       "      <td>0.850969</td>\n",
       "      <td>0.031720</td>\n",
       "      <td>0.023565</td>\n",
       "      <td>0.928465</td>\n",
       "      <td>0.012675</td>\n",
       "      <td>0.191919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.001132</td>\n",
       "      <td>0.048753</td>\n",
       "      <td>0.191919</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22825</th>\n",
       "      <td>8941286</td>\n",
       "      <td>0.229532</td>\n",
       "      <td>0.048750</td>\n",
       "      <td>0.850969</td>\n",
       "      <td>0.031720</td>\n",
       "      <td>0.023565</td>\n",
       "      <td>0.928465</td>\n",
       "      <td>0.012675</td>\n",
       "      <td>0.191919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.001110</td>\n",
       "      <td>0.048753</td>\n",
       "      <td>0.191919</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001110</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22826</th>\n",
       "      <td>8941452</td>\n",
       "      <td>0.229532</td>\n",
       "      <td>0.048750</td>\n",
       "      <td>0.850969</td>\n",
       "      <td>0.031720</td>\n",
       "      <td>0.023565</td>\n",
       "      <td>0.928465</td>\n",
       "      <td>0.012675</td>\n",
       "      <td>0.191919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.001163</td>\n",
       "      <td>0.048753</td>\n",
       "      <td>0.191919</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001163</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22827</th>\n",
       "      <td>8938949</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.048711</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011686</td>\n",
       "      <td>0.000234</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004670</td>\n",
       "      <td>0.070707</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.048711</td>\n",
       "      <td>0.070707</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22828</th>\n",
       "      <td>8942139</td>\n",
       "      <td>0.012199</td>\n",
       "      <td>0.048696</td>\n",
       "      <td>0.141580</td>\n",
       "      <td>0.398998</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.141580</td>\n",
       "      <td>0.159440</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.048020</td>\n",
       "      <td>0.048696</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.048020</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22829 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6   \\\n",
       "0      8853689  1.000000  0.246736  0.284650  0.165275  0.048804  0.248882   \n",
       "1      8863178  1.000000  0.246736  0.284650  0.165275  0.048804  0.248882   \n",
       "2      8863180  1.000000  0.246736  0.284650  0.165275  0.048804  0.248882   \n",
       "3      8863181  1.000000  0.246736  0.284650  0.165275  0.048804  0.248882   \n",
       "4      8926397  1.000000  0.246736  0.284650  0.165275  0.048804  0.356185   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "22824  8941283  0.229532  0.048750  0.850969  0.031720  0.023565  0.928465   \n",
       "22825  8941286  0.229532  0.048750  0.850969  0.031720  0.023565  0.928465   \n",
       "22826  8941452  0.229532  0.048750  0.850969  0.031720  0.023565  0.928465   \n",
       "22827  8938949  0.000006  0.048711  0.000000  0.011686  0.000234  0.000000   \n",
       "22828  8942139  0.012199  0.048696  0.141580  0.398998  1.000000  0.141580   \n",
       "\n",
       "             7         8         9         10        11        12        13  \\\n",
       "0      0.066044  1.000000  0.466667  0.308225  1.000000  0.247279  1.000000   \n",
       "1      0.066044  1.000000  0.800000  0.307232  0.001845  0.246101  1.000000   \n",
       "2      0.066044  1.000000  0.800000  0.303792  0.001203  0.241533  1.000000   \n",
       "3      0.066044  1.000000  0.400000  0.000226  0.002056  0.247444  1.000000   \n",
       "4      0.066044  1.000000  1.000000  0.306525  0.835991  0.248747  1.000000   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "22824  0.012675  0.191919  0.000000  0.000045  0.001132  0.048753  0.191919   \n",
       "22825  0.012675  0.191919  0.000000  0.000045  0.001110  0.048753  0.191919   \n",
       "22826  0.012675  0.191919  0.000000  0.000045  0.001163  0.048753  0.191919   \n",
       "22827  0.004670  0.070707  0.000000  0.000045  0.000011  0.048711  0.070707   \n",
       "22828  0.159440  0.030303  0.000000  0.000045  0.048020  0.048696  0.030303   \n",
       "\n",
       "         14    15        16        17   18   19   20   21   22   23   24   25  \\\n",
       "0      0.25  0.50  0.000000  1.000000  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0   \n",
       "1      0.25  0.50  0.000000  0.001845  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0   \n",
       "2      0.25  0.50  0.000000  0.001203  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0   \n",
       "3      0.25  0.50  0.000000  0.002056  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0   \n",
       "4      0.25  0.50  0.000000  0.835991  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0   \n",
       "...     ...   ...       ...       ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "22824  0.75  0.50  0.000000  0.001132  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0   \n",
       "22825  0.75  0.50  0.000000  0.001110  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0   \n",
       "22826  0.75  0.50  0.000000  0.001163  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0   \n",
       "22827  1.00  0.75  0.333333  0.000011  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "22828  0.25  0.25  0.000000  0.048020  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "        26   27   28  \n",
       "0      1.0  0.0  0.0  \n",
       "1      1.0  0.0  0.0  \n",
       "2      1.0  0.0  0.0  \n",
       "3      1.0  0.0  0.0  \n",
       "4      1.0  0.0  0.0  \n",
       "...    ...  ...  ...  \n",
       "22824  1.0  0.0  0.0  \n",
       "22825  1.0  0.0  0.0  \n",
       "22826  1.0  0.0  0.0  \n",
       "22827  0.0  1.0  0.0  \n",
       "22828  0.0  0.0  0.0  \n",
       "\n",
       "[22829 rows x 29 columns]"
      ]
     },
     "execution_count": 1393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_slurm_Cleaned_Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ## ---------------- ## ---------------- ## ----------------------- ## ------------------- ## ---------------------##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1394,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<AxesSubplot:title={'center':'MaxVMSize'}>]], dtype=object)"
      ]
     },
     "execution_count": 1394,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(slurm_Cleaned_Demo['MaxVMSize']).hist(bins=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1395,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(slurm_Cleaned_Demo['MaxVMSize'], bins=50, alpha=0.6, color='b')\n",
    "mu, std = norm.fit(slurm_Cleaned_Demo['MaxVMSize'])\n",
    "xmin, xmax = plt.xlim()\n",
    "x = np.linspace(xmin, xmax, 100)\n",
    "p = norm.pdf(x, mu, std)\n",
    "  \n",
    "plt.plot(x, p, 'k', linewidth=2)\n",
    "title = \"Fit Values: {:.2f} and {:.2f}\".format(mu, std)\n",
    "plt.title(title)\n",
    "  \n",
    "plt.show()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I will not consider the following features ###\n",
    "#### ReqNodes, TRESUsageOutMaxTask, ExitCode, billing, MaxDiskReadTask, MaxDiskWriteTask, MaxDiskWrite, MaxDiskRead, NTasks, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation and transformation for model features and targets ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1396,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdDf = pd.DataFrame({'TotalCPU':['227-18:12:13', '10:15.16', '19:14:17']})\n",
    "# pdDf['TotalCPU'].apply(lambda x: timedelta(days = float(x.split('-')[0]), hours = float(x.split('-')[1].split(':')[0]), minutes = float(x.split('-')[1].split(':')[1]), seconds = float(x.split('-')[1].split(':')[2])).total_seconds() if re.search(r\"(\\d+)-\", x) else (datetime.strptime('00:' + x.split(':')[0] + ':' + x.split(':')[1].split('.')[0], \"%H:%M:%S\") - datetime(1900, 1, 1)).total_seconds() if re.search(r\"(\\w+[:]\\w+)(\\w+[.]\\w+)\", x) else (datetime.strptime(x,'%H:%M:%S')- datetime(1900,1,1)).total_seconds())\n",
    "# slurm_Cleaned_Demo = slurm_Cleaned.head(10000).copy()\n",
    "# slurm_Cleaned_Demo['TotalCPU'] = slurm_Cleaned_Demo['TotalCPU'].apply(lambda x: timedelta(days = float(x.split('-')[0]), hours = float(x.split('-')[1].split(':')[0]), minutes = float(x.split('-')[1].split(':')[1]), seconds = float(x.split('-')[1].split(':')[2])).total_seconds() if re.search(r\"(\\d+)-\", x) else (datetime.strptime('00:' + x.split(':')[0] + ':' + x.split(':')[1].split('.')[0], \"%H:%M:%S\") - datetime(1900, 1, 1)).total_seconds() if re.search(r\"(\\w+[:]\\w+)(\\w+[.]\\w+)\", x) else (datetime.strptime(x,'%H:%M:%S')- datetime(1900,1,1)).total_seconds())\n",
    "# slurm_Cleaned_Demo['TotalCPU'] = slurm_Cleaned_Demo['TotalCPU'].multiply(slurm_Cleaned_Demo['NCPUS'], axis=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1397,
   "metadata": {},
   "outputs": [],
   "source": [
    "slurm_Cleaned_Demo.to_csv(\"/home/abose/HPC Analytics GCN/slurm_Cleaned_Demo.csv\", sep=',', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make unique Neumeric entry for \"Account\" and \"AssocID\" fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1398,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(demoDF[demoDF['failed'] == 0].index.values, type(demoDF[demoDF['failed'] == 0].index.values), type(demoDF[demoDF['failed'] == 0].index), len(demoDF[demoDF['failed'] == 0].index), len(demoDF[demoDF['failed'] == 1].index))\n",
    "# for i in demoDF.index.values:\n",
    "#     print(i, demoDF.loc[i, 'JobIDRaw'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following code block process the Target node lebels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1399,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_Save_Target_Data(args):\n",
    "    \"\"\"Loads dataset and graph if exists, else create and process them from raw data\n",
    "    Returns --->\n",
    "    selected: indexes of selected labelled nodes for training\n",
    "    test_idxs: indexes of not-selected nodes for inference/testing\n",
    "    labels_selected: labels of selected labelled nodes for training\n",
    "    labels_not_selected: labels of not-selected labelled nodes for inference/testing\n",
    "    \"\"\"\n",
    "    \n",
    "    demoDF=pd.read_csv('/home/abose/HPC Analytics GCN/slurm_Cleaned_Demo.csv', index_col=False)\n",
    "    ### stratified test samples\n",
    "    test_idxs = []\n",
    "    for status in demoDF['State'].unique():\n",
    "        dum = demoDF[demoDF['State'] == status].index.values\n",
    "        print(\"status-> \", status, \" dum-> \", len(dum))\n",
    "        test_idxs.extend(list(np.random.choice(dum, size=round(args['test_ratio']*len(dum)), replace=False)))\n",
    "\n",
    "    \n",
    "    # select only certain labelled nodes for semi-supervised GCN\n",
    "    print(\"test_idxs-> \", test_idxs)\n",
    "    selected = []\n",
    "    jobID_selected = list()\n",
    "    jobID_not_selected = list()\n",
    "    for i in demoDF.index.values:\n",
    "        if i not in test_idxs:\n",
    "            selected.append(i)\n",
    "            jobID_selected.append(demoDF.loc[i, 'JobID'])\n",
    "        if i in test_idxs:\n",
    "            jobID_not_selected.append(demoDF.loc[i, 'JobID'])\n",
    "    \n",
    "#     labels_all = [(l['JobID'], l['State']) for idx, l in demoDF.iterrows()] ##### FOR Classification #####\n",
    "#     labels_selected = [l['State'] for idx, l in demoDF.iterrows() if idx in selected] ##### FOR Classification #####\n",
    "#     labels_not_selected = [l['State'] for idx, l in demoDF.iterrows() if idx not in selected] ##### FOR Classification #####\n",
    "    \n",
    "      ##### Changed for Regression #####\n",
    "#     labels_all = [(l['JobID'], l['CPUTimeRAW']) for idx, l in demoDF.iterrows()]\n",
    "#     labels_selected = [l['CPUTimeRAW'] for idx, l in demoDF.iterrows() if idx in selected]\n",
    "#     labels_not_selected = [l['CPUTimeRAW'] for idx, l in demoDF.iterrows() if idx not in selected]\n",
    "    \n",
    "    labels_all = [(l['JobID'], l['MaxRSS']) for idx, l in demoDF.iterrows()]\n",
    "    labels_selected = [l['MaxRSS'] for idx, l in demoDF.iterrows() if idx in selected]\n",
    "    labels_not_selected = [l['MaxRSS'] for idx, l in demoDF.iterrows() if idx not in selected]\n",
    "      ##### Changed for Regression #####\n",
    "    \n",
    "    train_TargetDict = dict()\n",
    "    test_TargetDict = dict()\n",
    "    print(\"selected-> \", len(selected), \" labels_selected-> \", len(labels_selected), \" test_idxs-> \", len(test_idxs), \"labels_not_selected-> \", len(labels_not_selected))\n",
    "    train_TargetDict['Node_ID']=jobID_selected\n",
    "    train_TargetDict['Label']=labels_selected\n",
    "    test_TargetDict['Node_ID']= jobID_not_selected\n",
    "    test_TargetDict['Label']=labels_not_selected\n",
    "    targetDf=pd.DataFrame(labels_all, columns=['Node_ID', 'Label'])\n",
    "    train_TargetDf=pd.DataFrame(train_TargetDict)\n",
    "    test_TargetDf=pd.DataFrame(test_TargetDict)\n",
    "    targetDf.to_csv('/home/abose//HPC Analytics GCN/targetDf.csv', sep=',', index=False)\n",
    "    train_TargetDf.to_csv('/home/abose//HPC Analytics GCN/train_TargetDf.csv', sep=',', index=False)\n",
    "    test_TargetDf.to_csv('/home/abose//HPC Analytics GCN/test_TargetDf.csv', sep=',', index=False)\n",
    "    print(\"Split into %d train and %d test lebels.\" % (len(labels_selected), len(labels_not_selected)))\n",
    "    return selected, labels_selected, labels_not_selected, test_idxs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model's Data preparation ends here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation for Graph Generation ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The following code block defines a function that is used for bining purpose to assign values in different keys of above mentioned two dictionaries ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1400,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureSet_Group_Generator(Job, Flag, resource_dictionay):\n",
    "    if Flag==\"CPU\":\n",
    "        cpuList_Container_dictionary = resource_dictionay\n",
    "#         if Job['ReqCPUS']>0 and Job['ReqCPUS']<= 4:\n",
    "#             cpuList_Container_dictionary[\"mono1\"].append(Job['JobID'])\n",
    "#         elif Job['ReqCPUS']>4 and Job['ReqCPUS']<= 9:\n",
    "#             cpuList_Container_dictionary[\"mono2\"].append(Job['JobID'])\n",
    "#         elif Job['ReqCPUS']>9 and Job['ReqCPUS']<= 19:\n",
    "#             cpuList_Container_dictionary[\"decim1\"].append(Job['JobID'])\n",
    "#         elif Job['ReqCPUS']>19 and Job['ReqCPUS']<= 32:\n",
    "#             cpuList_Container_dictionary[\"decim2\"].append(Job['JobID'])\n",
    "#         elif Job['ReqCPUS']>32 and Job['ReqCPUS']<= 64:\n",
    "#             cpuList_Container_dictionary[\"decim3\"].append(Job['JobID'])\n",
    "#         elif Job['ReqCPUS']>64 and Job['ReqCPUS']<= 128:\n",
    "#             cpuList_Container_dictionary[\"centu1\"].append(Job['JobID'])\n",
    "#         elif Job['ReqCPUS']>128 and Job['ReqCPUS']<= 256:\n",
    "#             cpuList_Container_dictionary[\"centu2\"].append(Job['JobID'])\n",
    "#         if Job['ReqCPUS']>256 and Job['ReqCPUS']<= 512:\n",
    "#             cpuList_Container_dictionary[\"centu\"].append(Job['JobID'])\n",
    "        if Job['ReqCPUS']>500:\n",
    "            cpuList_Container_dictionary[\"centu4\"].append(Job['JobID'])\n",
    "        return cpuList_Container_dictionary\n",
    "    if Flag==\"Memory\":\n",
    "        memList_Container_dictionary = resource_dictionay\n",
    "#         if Job['ReqMem']>1 and Job['ReqMem'] <= 3:\n",
    "#             memList_Container_dictionary[\"squaric\"].append(Job['JobID'])\n",
    "#         elif Job['ReqMem'] > 3 and Job['ReqMem'] <= 7:\n",
    "#             memList_Container_dictionary[\"cubic\"].append(Job['JobID'])\n",
    "#         elif Job['ReqMem'] > 7 and Job['ReqMem'] <= 15:\n",
    "#             memList_Container_dictionary[\"quartic\"].append(Job['JobID'])\n",
    "#         elif Job['ReqMem'] > 15 and Job['ReqMem'] <= 31:\n",
    "#             memList_Container_dictionary[\"pentaic\"].append(Job['JobID'])\n",
    "#         elif Job['ReqMem'] > 31 and Job['ReqMem'] <= 63:\n",
    "#             memList_Container_dictionary[\"hexaic\"].append(Job['JobID'])\n",
    "#         elif Job['ReqMem'] > 63 and Job['ReqMem'] <= 127:\n",
    "#             memList_Container_dictionary[\"heptaic\"].append(Job['JobID'])\n",
    "#         elif Job['ReqMem'] > 127 and Job['ReqMem'] <= 256:\n",
    "#             memList_Container_dictionary[\"octaic\"].append(Job['JobID'])\n",
    "#         if Job['ReqMem'] > 256 and Job['ReqMem'] <= 512:\n",
    "#             memList_Container_dictionary[\"nonaic\"].append(Job['JobID'])\n",
    "        if Job['ReqMem'] > 500:\n",
    "            memList_Container_dictionary[\"decaic\"].append(Job['JobID'])\n",
    "        return memList_Container_dictionary\n",
    "    \n",
    "    if Flag==\"q5\":\n",
    "        q5List_Container_dictionary = resource_dictionay\n",
    "#         if Job['q5']== 1:\n",
    "#             q5List_Container_dictionary['beginner'].append(Job['JobID'])\n",
    "#         elif Job['q5']== 3 or Job['q5'] == 4:\n",
    "#             q5List_Container_dictionary['experienced'].append(Job['JobID'])\n",
    "        if Job['q5']== 5:\n",
    "            q5List_Container_dictionary['expert'].append(Job['JobID'])\n",
    "        return q5List_Container_dictionary\n",
    "    \n",
    "    if Flag==\"q6\":\n",
    "        q6List_Container_dictionary = resource_dictionay\n",
    "#         if Job['q6']== 1:\n",
    "#             q6List_Container_dictionary['beginner'].append(Job['JobID'])\n",
    "        if Job['q6'] == 5:\n",
    "            q6List_Container_dictionary['expert'].append(Job['JobID'])\n",
    "        return q6List_Container_dictionary\n",
    "    \n",
    "    if Flag==\"q7\":\n",
    "        q7List_Container_dictionary = resource_dictionay\n",
    "#         if Job['q7']== 1:\n",
    "#             q7List_Container_dictionary['weakly_trained'].append(Job['JobID'])\n",
    "        if Job['q7'] == 4:\n",
    "            q7List_Container_dictionary['trained'].append(Job['JobID'])\n",
    "        return q7List_Container_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The following function is used to calculate the (node, node) tuple from the dictionaries created by above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1401,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def implicit_Relation_Maker(List_Container_dictionary):\n",
    "#     element_element=list()\n",
    "#     for key, itemList in List_Container_dictionary.items():\n",
    "#         for idx, element in enumerate(itemList):\n",
    "#             for x in range(idx+1, len(itemList)-1):\n",
    "#                 element_element.append((str(element), str(itemList[x])))\n",
    "#     return element_element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1402,
   "metadata": {},
   "outputs": [],
   "source": [
    "def implicit_Relation_Maker(List_Container_dictionary, dummy_node):\n",
    "    element_element=list()\n",
    "    for key, itemList in List_Container_dictionary.items():\n",
    "        for idx, element in enumerate(itemList):\n",
    "            element_element.append((str(element), dummy_node))\n",
    "    return element_element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1403,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_data_builder():\n",
    "    #### The following code block defines a dictionary with different keys for bining purpose to constract a heterogenous graph ####\n",
    "    cpuList_Container_dictionary = dict()\n",
    "#     cpuList_Container_dictionary[\"mono1\"]=list()\n",
    "#     cpuList_Container_dictionary[\"mono2\"]=list()\n",
    "#     cpuList_Container_dictionary[\"decim1\"]=list()\n",
    "#     cpuList_Container_dictionary[\"decim2\"]=list()\n",
    "#     cpuList_Container_dictionary[\"decim3\"]=list()\n",
    "#     cpuList_Container_dictionary[\"centu1\"]=list()\n",
    "#     cpuList_Container_dictionary[\"centu2\"]=list()\n",
    "#     cpuList_Container_dictionary[\"centu\"]=list()\n",
    "    cpuList_Container_dictionary[\"centu4\"]=list()\n",
    "\n",
    "    memList_Container_dictionary = dict()\n",
    "#     memList_Container_dictionary[\"squaric\"]=list()\n",
    "#     memList_Container_dictionary[\"cubic\"]=list()\n",
    "#     memList_Container_dictionary[\"quartic\"]=list()\n",
    "#     memList_Container_dictionary[\"pentaic\"]=list()\n",
    "#     memList_Container_dictionary[\"hexaic\"]=list()\n",
    "#     memList_Container_dictionary[\"heptaic\"]=list()\n",
    "#     memList_Container_dictionary[\"octaic\"]=list()\n",
    "#     memList_Container_dictionary[\"nonaic\"]=list()\n",
    "    memList_Container_dictionary[\"decaic\"]=list()\n",
    "    \n",
    "    q5List_Container_dictionary = dict()\n",
    "#     q5List_Container_dictionary['beginner'] = list()\n",
    "#     q5List_Container_dictionary['experienced'] = list()\n",
    "    q5List_Container_dictionary['expert'] = list()\n",
    "    \n",
    "    q6List_Container_dictionary = dict()\n",
    "#     q6List_Container_dictionary['beginner'] = list()\n",
    "    q6List_Container_dictionary['expert'] = list()\n",
    "    \n",
    "    q7List_Container_dictionary = dict()\n",
    "#     q7List_Container_dictionary['weakly_trained'] = list()\n",
    "    q7List_Container_dictionary['trained'] = list()\n",
    "    \n",
    "\n",
    "    #### The following code block iterates the datasets to create tuples of (node, node, weight) for both implicit and\n",
    "    #### explicit relations between different types of nodes.\n",
    "#     AssocID_jobID=list()\n",
    "#     AssocID_JobName=list()\n",
    "#     JobName_jobID = list()\n",
    "#     NodeList_jobID = list()\n",
    "    UID_jobID = list()\n",
    "#     GID_UID = list()\n",
    "#     UID_department = list()\n",
    "#     GID_department = list()\n",
    "#     GID_AssocID = list()\n",
    "    for index, row in slurm_Cleaned_Demo.iterrows():\n",
    "        cpuList_Container_dictionary = featureSet_Group_Generator(row, \"CPU\", cpuList_Container_dictionary)\n",
    "        memList_Container_dictionary = featureSet_Group_Generator(row, \"Memory\", memList_Container_dictionary)\n",
    "        \n",
    "        q5List_Container_dictionary = featureSet_Group_Generator(row, \"q5\", q5List_Container_dictionary)\n",
    "        q6List_Container_dictionary = featureSet_Group_Generator(row, \"q6\", q6List_Container_dictionary)\n",
    "        q7List_Container_dictionary = featureSet_Group_Generator(row, \"q7\", q7List_Container_dictionary)\n",
    "        \n",
    "#         JobName_jobID.append((str(row['JobName']), str(row['JobID'])))\n",
    "#         AssocID_jobID.append((str(row['AssocID']), str(row['JobID'])))\n",
    "#         AssocID_JobName.append((str(row['AssocID']), str(row['JobName'])))\n",
    "#         NodeList_jobID.append((row['NodeList'], row['JobID']))\n",
    "        UID_jobID.append((str(row['UID']), str(row['JobID'])))\n",
    "#         GID_UID.append((str(row['GID']), str(row['UID'])))\n",
    "#         UID_department.append((str(row['UID']), str(row['department'])))\n",
    "#         GID_department.append((str(row['GID']), str(row['department'])))\n",
    "#         GID_AssocID.append((str(row['GID']), str(row['AssocID'])))\n",
    "\n",
    "    #### The following code block merges several dataframes and generates concateneted \n",
    "    #### Nodes are 'AssocID', 'Account', 'JobID', 'JobName', 'NodeList, GID, UID, department'\n",
    "#     JobName_jobID_DF= pd.DataFrame(JobName_jobID, columns = ['Src', 'Des'])\n",
    "#     AssocID_jobID_DF = pd.DataFrame(AssocID_jobID, columns =['Src', 'Des'])\n",
    "#     AssocID_JobName_DF = pd.DataFrame(AssocID_JobName, columns =['Src', 'Des'])\n",
    "#     NodeList_jobID_DF= pd.DataFrame(NodeList_jobID, columns = ['Src', 'Des'])\n",
    "    UID_jobID_DF= pd.DataFrame(UID_jobID, columns = ['Src', 'Des'])\n",
    "#     GID_UID_DF= pd.DataFrame(GID_UID, columns = ['Src', 'Des'])\n",
    "#     UID_department_DF= pd.DataFrame(UID_department, columns = ['Src', 'Des'])\n",
    "#     GID_department_DF= pd.DataFrame(GID_department, columns = ['Src', 'Des'])\n",
    "#     GID_AssocID_DF= pd.DataFrame(GID_AssocID, columns = ['Src', 'Des'])\n",
    "    cpu_JobID_JobID_DF= pd.DataFrame(implicit_Relation_Maker(cpuList_Container_dictionary, 'V'), columns = ['Src', 'Des'])\n",
    "    mem_JobID_JobID_DF= pd.DataFrame(implicit_Relation_Maker(memList_Container_dictionary, 'W'), columns = ['Src', 'Des'])\n",
    "    \n",
    "    q5_JobID_JobID_DF= pd.DataFrame(implicit_Relation_Maker(q5List_Container_dictionary, 'X'), columns = ['Src', 'Des'])\n",
    "    q6_JobID_JobID_DF= pd.DataFrame(implicit_Relation_Maker(q6List_Container_dictionary, 'Y'), columns = ['Src', 'Des'])\n",
    "    q7_JobID_JobID_DF= pd.DataFrame(implicit_Relation_Maker(q7List_Container_dictionary, 'Z'), columns = ['Src', 'Des'])\n",
    "    \n",
    "#     graph_DF = pd.concat([AssocID_jobID_DF, UID_jobID_DF, GID_UID_DF, UID_department_DF, GID_department_DF, GID_AssocID_DF, cpu_JobID_JobID_DF, mem_JobID_JobID_DF, q5_JobID_JobID_DF, q6_JobID_JobID_DF, q7_JobID_JobID_DF])\n",
    "#     graph_DF = pd.concat([AssocID_jobID_DF, UID_jobID_DF, GID_UID_DF, UID_department_DF, GID_department_DF, GID_AssocID_DF, cpu_JobID_JobID_DF, mem_JobID_JobID_DF, q5_JobID_JobID_DF, q6_JobID_JobID_DF, q7_JobID_JobID_DF])\n",
    "    graph_DF = pd.concat([UID_jobID_DF, cpu_JobID_JobID_DF, mem_JobID_JobID_DF, q5_JobID_JobID_DF, q6_JobID_JobID_DF, q7_JobID_JobID_DF])\n",
    "    print(\"graph_DF DataType-> \", graph_DF.dtypes)\n",
    "    graph_DF.to_csv(\"/home/abose/HPC Analytics GCN/graph_DF.csv\", sep=',', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation for Graph Ends ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Generation Process Starts from here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mapping heterogenous nodes to numeric values ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1404,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nxNode_to_Index_Mapping = {node:ind for ind, node in enumerate(list(nx_graph.nodes()))}\n",
    "# index_to_nxNode_Mapping = {ind:node for ind, node in enumerate(list(nx_graph.nodes()))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1405,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count_graph=nx.read_edgelist('/home/abose/HPC Analytics GCN/graph_DF.csv', delimiter=',') ##### FOR Classification #####\n",
    "# print(\"Job Node-> \", len(set(slurm_Cleaned_Demo['JobID'])), \" UID Node-> \", len(set(slurm_Cleaned_Demo['UID'])), \" GID Node-> \", len(set(slurm_Cleaned_Demo['GID'])), \" No of Edges-> \", len(count_graph.edges()), \" Average Degree-> \", sum(dict(count_graph.degree()).values())/len(count_graph.nodes())) ##### FOR Classification #####\n",
    "# print((len(set(slurm_Cleaned_Demo['JobID']))+len(set(slurm_Cleaned_Demo['UID']))+len(set(slurm_Cleaned_Demo['GID'])))*0.8,(len(set(slurm_Cleaned_Demo['JobID']))+len(set(slurm_Cleaned_Demo['UID']))+len(set(slurm_Cleaned_Demo['GID'])))*0.2) ##### FOR Classification #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1406,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_obj(name ):\n",
    "    with open('obj/' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1407,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_mapped_Embedding_for_Nodes(nx_graph, index_node_mapping):\n",
    "    normalized_slurm_Cleaned_dictionary = load_obj('slurm_job_feature_dictionary')\n",
    "    node_Emedding_Dict = OrderedDict()\n",
    "    node_List= list(nx_graph.nodes())\n",
    "    \n",
    "    for idx, node_element in enumerate(node_List):\n",
    "        if index_node_mapping[node_element] in normalized_slurm_Cleaned_dictionary:\n",
    "            node_Emedding_Dict[node_element] = normalized_slurm_Cleaned_dictionary[index_node_mapping[node_element]]\n",
    "        else:\n",
    "            node_Emedding_Dict[node_element]=np.random.rand(normalized_slurm_Cleaned_Demo.shape[1]-1)\n",
    "    \n",
    "    embedding_DataFrame=pd.DataFrame(node_Emedding_Dict)\n",
    "    embedding_DataFrame.to_csv('/home/abose/HPC Analytics GCN/embedding_Dataframe.csv', sep=',')\n",
    "    print(\"embedding_DataFrame.shape-> \", embedding_DataFrame.shape)\n",
    "    return embedding_DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1408,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_Embedding_for_Nodes(nx_graph):\n",
    "#     node_Emedding_Dict = OrderedDict()\n",
    "#     id_matrix = np.eye(len(nx_graph.nodes()))\n",
    "#     node_List= list(nx_graph.nodes())\n",
    "#     for idx, node_element in enumerate(node_List):\n",
    "#         node_Emedding_Dict[node_element] = id_matrix[idx]\n",
    "#     embedding_DataFrame=pd.DataFrame(node_Emedding_Dict)\n",
    "#     embedding_DataFrame.to_csv('/home/abose/HPC Analytics GCN/embedding_Dataframe.csv', sep=',')\n",
    "#     print(\"embedding_DataFrame.shape-> \", embedding_DataFrame.shape)\n",
    "#     return embedding_DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1409,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def node_embedding_mapper(normalized_slurm_Cleaned_dictionary, word_Value_Dict):\n",
    "#     complete_slurm_feature_dictionary=pd.DataFrame()\n",
    "#     for item, value in word_Value_Dict.items():\n",
    "#         if item in normalized_slurm_Cleaned_dictionary:\n",
    "#             complete_slurm_feature_dictionary[value]=normalized_slurm_Cleaned_dictionary[item]\n",
    "#         else:\n",
    "#             print(\"word not in new embed df-> \", item)\n",
    "#             complete_slurm_feature_dictionary[value]=np.random.rand(28)\n",
    "#     return complete_slurm_feature_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The following code block convert string nodes label to neumeric node labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1410,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neumeric_node_mapper():\n",
    "    src_des_edges=pd.read_csv('/home/abose/HPC Analytics GCN/graph_DF.csv')\n",
    "    totalNodeList=list(set(src_des_edges['Src']).union(set(src_des_edges['Des'])))\n",
    "    node_neumeric_mapping = {str(node): i for i, node in enumerate(totalNodeList)}\n",
    "    index_node_mapping = {i: str(node) for i, node in enumerate(totalNodeList)}\n",
    "    neumeric_node_node_Dict=dict()\n",
    "    src_List=list()\n",
    "    des_List=list()\n",
    "    score_List=list()\n",
    "    for indx, row in src_des_edges.iterrows():\n",
    "        row=dict(row)\n",
    "        if str(row['Src']) in node_neumeric_mapping and str(row['Des']) in node_neumeric_mapping:\n",
    "            src_List.append(node_neumeric_mapping[str(row['Src'])])\n",
    "            des_List.append(node_neumeric_mapping[str(row['Des'])])\n",
    "    neumeric_node_node_Dict['Src']=src_List\n",
    "    neumeric_node_node_Dict['Des']=des_List\n",
    "    src_des_edges=pd.DataFrame(neumeric_node_node_Dict)\n",
    "    node_neumeric_mapping_DF= pd.DataFrame(list(node_neumeric_mapping.items()),columns = ['Node_ID','Assigned_ID'])\n",
    "    node_neumeric_mapping_DF.to_csv('/home/abose/HPC Analytics GCN/node_neumeric_mapping.csv', sep=',', index=False)\n",
    "    src_des_edges.to_csv('/home/abose/HPC Analytics GCN/src_des_edges.csv', header=False, sep=',', index=False)\n",
    "    return node_neumeric_mapping, index_node_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping the Target dataframe index to numeric identifier that has been indexed by the jobID earlier ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1411,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_mapper(target, nodeMap, data_chunk):\n",
    "    newTargetNode=list()\n",
    "    newTargetLabel=list()\n",
    "    newTargetDict=dict()\n",
    "    for index, item in target.iterrows():\n",
    "        if str(item[\"Node_ID\"]) in nodeMap:\n",
    "#             print(\"-> \", item[\"Node_ID\"])\n",
    "            newTargetNode.append(nodeMap[str(item[\"Node_ID\"])])\n",
    "            newTargetLabel.append(item[\"Label\"])\n",
    "    newTargetDict[\"Node_ID\"] = newTargetNode\n",
    "    newTargetDict[\"Label\"] = newTargetLabel\n",
    "    newTarget = pd.DataFrame(newTargetDict)\n",
    "    if data_chunk == 0:\n",
    "        newTarget.to_csv(\"/home/abose//HPC Analytics GCN/new_Target.csv\", sep=',', index=False)\n",
    "    if data_chunk == 1:\n",
    "        newTarget.to_csv(\"/home/abose//HPC Analytics GCN/new_Train_Target.csv\", sep=',', index=False)\n",
    "    if data_chunk == 2:\n",
    "        newTarget.to_csv(\"/home/abose//HPC Analytics GCN/new_Test_Target.csv\", sep=',', index=False)\n",
    "    return newTarget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The following line finally creates a graph from the saved CSV file from the concatenated dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1412,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_generation():\n",
    "    graph_data_builder()\n",
    "    node_neumeric_mapping, index_node_mapping = neumeric_node_mapper()\n",
    "    nx_graph=nx.read_edgelist('/home/abose/HPC Analytics GCN/src_des_edges.csv', delimiter=',', nodetype=int)\n",
    "#     print(nx.info(nx_graph))\n",
    "#     nx.draw(nx_graph)\n",
    "#     plt.show()\n",
    "    return nx_graph, node_neumeric_mapping, index_node_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Generation Ends ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Convolutional Part ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1413,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackedGCN(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-layer GCN model.\n",
    "    \"\"\"\n",
    "    def __init__(self, args, input_channels, output_channels):\n",
    "        \"\"\"\n",
    "        :param args: Arguments object.\n",
    "        :input_channels: Number of features.\n",
    "        :output_channels: Number of target features. \n",
    "        \"\"\"\n",
    "        super(StackedGCN, self).__init__()\n",
    "        self.args = args\n",
    "        self.input_channels = input_channels\n",
    "        self.output_channels = output_channels\n",
    "        self.setup_layers()\n",
    "        \n",
    "    def setup_layers(self):\n",
    "        \"\"\"\n",
    "        Creating the layes based on the args.\n",
    "        \"\"\"\n",
    "        self.layers = []\n",
    "        self.args['layers'] = [self.input_channels] + self.args['layers'] + [self.output_channels]\n",
    "        for i, _ in enumerate(self.args['layers'][:-1]):\n",
    "            self.layers.append(GCNConv(self.args['layers'][i],self.args['layers'][i+1]))\n",
    "        self.layers = ListModule(*self.layers)\n",
    "\n",
    "#     def forward(self, edges, features, weights):\n",
    "    def forward(self, edges, features):\n",
    "        \"\"\"\n",
    "        Making a forward pass.\n",
    "        :param edges: Edge list LongTensor.\n",
    "        :param features: Feature matrix input FLoatTensor.\n",
    "        :return predictions: Prediction matrix output FLoatTensor.\n",
    "        \"\"\"\n",
    "        for i, _ in enumerate(self.args['layers'][:-2]):\n",
    "#             features = torch.nn.functional.relu(self.layers[i](features, edges, weights))\n",
    "            features = torch.nn.functional.relu(self.layers[i](features, edges))\n",
    "            if i>1:\n",
    "                features = torch.nn.functional.dropout(features, p = self.args['dropout'], training = self.training)\n",
    "#         features = self.layers[i+1](features, edges, weights)\n",
    "        features = self.layers[i+1](features, edges)\n",
    "#         predictions = torch.nn.functional.log_softmax(features, dim=1) ##### FOR Classification ##### ???\n",
    "#         predictions = torch.nn.functional.linear(features) ##### Changed for Regression #####\n",
    "        predictions = features ##### Changed for Regression #####\n",
    "#         predictions = torch.nn.functional.sigmoid(features)\n",
    "        print(\"StackedGCN Forward pred size-> \", predictions.size)\n",
    "        return predictions\n",
    "\n",
    "class ListModule(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Abstract list layer class.\n",
    "    \"\"\"\n",
    "    def __init__(self, *args):\n",
    "        \"\"\"\n",
    "        Module initializing.\n",
    "        \"\"\"\n",
    "        super(ListModule, self).__init__()\n",
    "        idx = 0\n",
    "        for module in args:\n",
    "            self.add_module(str(idx), module)\n",
    "            idx += 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Getting the indexed layer.\n",
    "        \"\"\"\n",
    "        if idx < 0 or idx >= len(self._modules):\n",
    "            raise IndexError('index {} is out of range'.format(idx))\n",
    "        it = iter(self._modules.values())\n",
    "        for i in range(idx):\n",
    "            next(it)\n",
    "        return next(it)\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        Iterating on the layers.\n",
    "        \"\"\"\n",
    "        return iter(self._modules.values())\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Number of layers.\n",
    "        \"\"\"\n",
    "        return len(self._modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1414,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClusteringMachine(object):\n",
    "    \"\"\"\n",
    "    Clustering the graph, feature set and target.\n",
    "    \"\"\"\n",
    "    def __init__(self, args, graph, features, target, train_Target, test_Target):\n",
    "        \"\"\"\n",
    "        :param args: Arguments object with parameters.\n",
    "        :param graph: Networkx Graph.\n",
    "        :param features: Feature matrix (ndarray).\n",
    "        :param target: Target vector (ndarray).\n",
    "        \"\"\"\n",
    "        self.args = args\n",
    "        self.graph = graph\n",
    "        self.features = features\n",
    "#         print(\"cls Machine features-> \", features)\n",
    "        self.target = target\n",
    "        self.train_Target = train_Target\n",
    "        self.test_Target = test_Target\n",
    "        self._set_sizes()\n",
    "\n",
    "    def _set_sizes(self):\n",
    "        \"\"\"\n",
    "        Setting the feature and class count.\n",
    "        \"\"\"\n",
    "        self.feature_count = self.features.shape[0]\n",
    "#         self.class_count = np.max(self.target['Label'])+1 ##### FOR Classification #####\n",
    "        self.class_count = 1 ##### Changed for Regression #####\n",
    "\n",
    "    def decompose(self):\n",
    "        \"\"\"\n",
    "        Decomposing the graph, partitioning the features and target, creating Torch arrays.\n",
    "        \"\"\"\n",
    "        if self.args['clustering_method'] == \"metis\":\n",
    "            print(\"\\nMetis graph clustering started.\\n\")\n",
    "            self.metis_clustering()\n",
    "        else:\n",
    "            print(\"\\nRandom graph clustering started.\\n\")\n",
    "            self.random_clustering()\n",
    "        self.general_data_partitioning()\n",
    "        self.transfer_edges_and_nodes()\n",
    "\n",
    "    def random_clustering(self):\n",
    "        \"\"\"\n",
    "        Random clustering the nodes.\n",
    "        \"\"\"\n",
    "        self.clusters = [cluster for cluster in range(self.args['cluster_no'])]\n",
    "        self.cluster_membership = {node: random.choice(self.clusters) for node in self.graph.nodes()}\n",
    "\n",
    "    def metis_clustering(self):\n",
    "        \"\"\"\n",
    "        Clustering the graph with Metis. For details see:\n",
    "        \"\"\"\n",
    "        (st, parts) = metis.part_graph(self.graph, self.args['cluster_no'])\n",
    "        self.clusters = list(set(parts))\n",
    "#         print(\"parts------->>>>> \", len(parts), \" self.graph.nodes------->>>>> \", len(self.graph.nodes()), \" +++ \", self.graph.nodes())\n",
    "#         for node, membership in enumerate(parts):\n",
    "#             print(\"node-> \", int(node)+1, \" membership-> \", membership)\n",
    "        graph_Node_List = list(self.graph.nodes())\n",
    "#         ********* My GCN WORK SHOULD DEFINITELY NOTICE IT ********** \n",
    "        self.cluster_membership = {graph_Node_List[node]: membership for node, membership in enumerate(parts)}\n",
    "        with open('cluster_membership.json', 'w') as fp:\n",
    "            json.dump(self.cluster_membership, fp)\n",
    "\n",
    "    def general_data_partitioning(self):\n",
    "        \"\"\"\n",
    "        Creating data partitions and train-test splits.\n",
    "        \"\"\"\n",
    "        self.sg_nodes = {}\n",
    "        self.sg_edges = {}\n",
    "#         self.sg_edge_weights = {}\n",
    "        self.sg_train_nodes = {}\n",
    "        self.sg_test_nodes = {}\n",
    "        self.sg_features = {}\n",
    "        # self.sg_targets = {}\n",
    "        self.sg_train_targets ={}\n",
    "        self.sg_test_targets ={}\n",
    "        self.sg_train_target_nodes = {}\n",
    "        self.sg_test_target_nodes = {}\n",
    "        # print(\"self.clusters-> \", self.clusters, \" self.cluster_membership-> \", self.cluster_membership)\n",
    "        for cluster in self.clusters:\n",
    "            subgraph = self.graph.subgraph([node for node in sorted(self.graph.nodes()) if self.cluster_membership[int(node)] == cluster])\n",
    "            self.sg_nodes[cluster] = [node for node in sorted(subgraph.nodes())]\n",
    "            # print(\" Check 1st sub nodes-> \", self.sg_nodes[cluster], \" Check 1st sub edges-> \",  subgraph.edges())\n",
    "            mapper = {node: i for i, node in enumerate(sorted(self.sg_nodes[cluster]))} \n",
    "            edge_data = [[mapper[edge[0]], mapper[edge[1]]] for edge in subgraph.edges()] +  [[mapper[edge[1]], mapper[edge[0]]] for edge in subgraph.edges()]\n",
    "            self.sg_edges[cluster] = [[elements[0], elements[1]] for i, elements in enumerate(edge_data)]\n",
    "#             self.sg_edge_weights[cluster] = [elements[2] for i, elements in enumerate(edge_data)]\n",
    "            self.sg_train_nodes[cluster], self.sg_test_nodes[cluster] = train_test_split(list(mapper.values()), test_size = self.args['test_ratio'])\n",
    "            self.sg_test_nodes[cluster] = sorted(self.sg_test_nodes[cluster])\n",
    "            self.sg_train_nodes[cluster] = sorted(self.sg_train_nodes[cluster])\n",
    "            self.sg_features[cluster] = self.features[self.sg_nodes[cluster]].values.T\n",
    "\n",
    "            # The following lists will be used as temporary container for keeping mapped Train and Test sample node ids in the current subgraph by checking nodes' existance in the Target dictionary\n",
    "            trainTarget=list()\n",
    "            testTarget=list()\n",
    "\n",
    "            copy_temp_target = self.target.copy()\n",
    "            # Checking whether target nodes are in the mapper dictionary or not for this Subgraph or Cluster of nodes // Discard Null Values that are not matched\n",
    "            copy_temp_target = copy_temp_target[copy_temp_target['Node_ID'].isin(mapper.keys())]\n",
    "\n",
    "            # Replace target dataframe Node_ID column by mapping with mapper dictionary\n",
    "            copy_temp_target = copy_temp_target.replace({'Node_ID': mapper})\n",
    "\n",
    "            # Making a dictionary from the target dataframe where Node_ID is the key and Label is the value\n",
    "            target_dict=copy_temp_target.set_index('Node_ID')['Label'].to_dict()\n",
    "\n",
    "            # Getting a list of Label values by mapping keys from subgraph train+test nodes cluster with target dictionary \n",
    "            self.sg_train_targets[cluster] = [target_dict.get(key) for key in self.sg_train_nodes[cluster]]\n",
    "            self.sg_test_targets[cluster] = [target_dict.get(key) for key in self.sg_test_nodes[cluster]]\n",
    "  \n",
    "            # Removing None from the Target labels list\n",
    "            self.sg_train_targets[cluster] = [i for i in self.sg_train_targets[cluster] if i is not None]\n",
    "            self.sg_test_targets[cluster] = [i for i in self.sg_test_targets[cluster] if i is not None]\n",
    "            \n",
    "            # Converting from List of numeric values to an one dimentional array\n",
    "            self.sg_train_targets[cluster] = np.array(self.sg_train_targets[cluster])\n",
    "            self.sg_test_targets[cluster] = np.array(self.sg_test_targets[cluster])\n",
    "\n",
    "            # Transposing the array of both Train and Test Matrix\n",
    "            self.sg_train_targets[cluster] = self.sg_train_targets[cluster].reshape(-1,1)\n",
    "            self.sg_test_targets[cluster] = self.sg_test_targets[cluster].reshape(-1,1)\n",
    "\n",
    "            # Checking whether a node in Train or Test Cluster is in the target dictionary or not \n",
    "            if len(self.sg_train_targets[cluster])>0:\n",
    "                for nodeId in self.sg_train_nodes[cluster]:\n",
    "                    if nodeId in target_dict.keys():\n",
    "                        trainTarget.append(nodeId)\n",
    "            if len(self.sg_test_targets[cluster])>0:\n",
    "                for nodeId in self.sg_test_nodes[cluster]:\n",
    "                    if nodeId in target_dict.keys():\n",
    "                        testTarget.append(nodeId)\n",
    "\n",
    "            self.sg_train_target_nodes[cluster] = trainTarget\n",
    "            self.sg_test_target_nodes[cluster] = testTarget\n",
    "            # print(\"Node id diff -> \", set(self.sg_train_target_nodes[cluster])-set(target_dict.keys()),\" Target key diff -> \", set(target_dict.keys())-set(self.sg_train_target_nodes[cluster]))\n",
    "            print(\"Target Train Array Shape-> \", self.sg_train_targets[cluster].shape, \" Target Test Array Shape-> \", self.sg_test_targets[cluster].shape)\n",
    "\n",
    "    def transfer_edges_and_nodes(self):\n",
    "        \"\"\"\n",
    "        Transfering the data to PyTorch format.\n",
    "        \"\"\"\n",
    "        for cluster in self.clusters:\n",
    "#             if not self.sg_edges[cluster] and self.sg_edge_weights[cluster]:\n",
    "            if not self.sg_edges[cluster]:\n",
    "                continue\n",
    "            self.sg_nodes[cluster] = torch.LongTensor(self.sg_nodes[cluster])\n",
    "            self.sg_edges[cluster] = torch.LongTensor(self.sg_edges[cluster]).t()\n",
    "#             self.sg_edge_weights[cluster] = torch.FloatTensor(self.sg_edge_weights[cluster]).t()\n",
    "            self.sg_train_target_nodes[cluster] = torch.LongTensor(self.sg_train_target_nodes[cluster])\n",
    "            self.sg_test_target_nodes[cluster] = torch.LongTensor(self.sg_test_target_nodes[cluster])\n",
    "            self.sg_train_targets[cluster] = torch.LongTensor(self.sg_train_targets[cluster])\n",
    "            self.sg_test_targets[cluster] = torch.LongTensor(self.sg_test_targets[cluster])\n",
    "            self.sg_features[cluster] = torch.FloatTensor(self.sg_features[cluster])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1415,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClusterGCNTrainer(object):\n",
    "    \"\"\"\n",
    "    Training a ClusterGCN.\n",
    "    \"\"\"\n",
    "    def __init__(self, args, clustering_machine):\n",
    "        \"\"\"\n",
    "        :param ags: Arguments object.\n",
    "        :param clustering_machine:\n",
    "        \"\"\"  \n",
    "        self.args = args\n",
    "        self.clustering_machine = clustering_machine\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.create_model()\n",
    "\n",
    "    def create_model(self):\n",
    "        \"\"\"\n",
    "        Creating a StackedGCN and transferring to CPU/GPU.\n",
    "        \"\"\"\n",
    "        print(\"feature_count_1-> \", self.clustering_machine.feature_count,\" class_count_1-> \", self.clustering_machine.class_count)\n",
    "        self.model = StackedGCN(self.args, self.clustering_machine.feature_count, self.clustering_machine.class_count)\n",
    "        self.model = self.model.to(self.device)\n",
    "\n",
    "    def do_forward_pass(self, cluster):\n",
    "        \"\"\"\n",
    "        Making a forward pass with data from a given partition.\n",
    "        :param cluster: Cluster index.\n",
    "        :return average_loss: Average loss on the cluster.\n",
    "        :return node_count: Number of nodes.\n",
    "        \"\"\"\n",
    "        print(\"Know the Type-> \", type(self.clustering_machine.sg_edges[cluster]), len(self.clustering_machine.sg_edges[cluster]))\n",
    "        edges = self.clustering_machine.sg_edges[cluster].to(self.device)\n",
    "#         weights = self.clustering_machine.sg_edge_weights[cluster].to(self.device)\n",
    "        macro_nodes = self.clustering_machine.sg_nodes[cluster].to(self.device)\n",
    "        train_nodes = self.clustering_machine.sg_train_target_nodes[cluster].to(self.device)\n",
    "        features = self.clustering_machine.sg_features[cluster].to(self.device)\n",
    "        target = torch.squeeze(self.clustering_machine.sg_train_targets[cluster].to(self.device), 1)\n",
    "        # target = self.clustering_machine.sg_train_targets[cluster].to(self.device).squeeze()\n",
    "#         predictions = self.model(edges, features, weights)\n",
    "        predictions = self.model(edges, features)\n",
    "        print(\"*** Check Carefully ***\", \"Input Train Nodes-> \", predictions[train_nodes].size(), \" Target Train Nodes-> \", target.size())\n",
    "#         print(\"predictions[train_nodes]-> \", len(predictions[train_nodes]), \" Target Train Nodes-> \", len(target))\n",
    "#         average_loss = torch.nn.functional.nll_loss(predictions[train_nodes], target) ##### For Classification #####\n",
    "        average_loss = torch.nn.functional.mse_loss(predictions[train_nodes], target.to(torch.float32)) ##### Changed for Regression #####\n",
    "        print(\"Avg MSE Loss-> \", average_loss) ##### Changed for Regression #####\n",
    "#         average_loss = torch.nn.functional.binary_cross_entropy(predictions[train_nodes], target)\n",
    "#         print(\"Eikhane Dekh -> \",  type(predictions[train_nodes]), predictions[train_nodes], \" *** \", type(target), target)\n",
    "#         check = torch.argmax(predictions[train_nodes,:], dim=1)\n",
    "#         clust_acc_score= accuracy_score(torch.argmax(predictions[train_nodes,:], dim=1).cpu().detach().numpy(), target.cpu().detach().numpy()) ##### For Accuracy #####\n",
    "        node_count = train_nodes.shape[0]\n",
    "#         return clust_acc_score, average_loss, node_count ##### For Accuracy #####\n",
    "        return average_loss, node_count\n",
    "\n",
    "    def update_average_loss(self, batch_average_loss, node_count):\n",
    "        \"\"\"\n",
    "        Updating the average loss in the epoch.\n",
    "        :param batch_average_loss: Loss of the cluster. \n",
    "        :param node_count: Number of nodes in currently processed cluster.\n",
    "        :return average_loss: Average loss in the epoch.\n",
    "        \"\"\"\n",
    "        self.accumulated_training_loss = self.accumulated_training_loss + batch_average_loss.item()*node_count\n",
    "        self.node_count_seen = self.node_count_seen + node_count\n",
    "        average_loss = self.accumulated_training_loss/self.node_count_seen\n",
    "        return average_loss\n",
    "\n",
    "    def do_prediction(self, cluster):\n",
    "        \"\"\"\n",
    "        Scoring a cluster.\n",
    "        :param cluster: Cluster index.\n",
    "        :return prediction: Prediction matrix with probabilities.\n",
    "        :return target: Target vector.\n",
    "        \"\"\"\n",
    "        edges = self.clustering_machine.sg_edges[cluster].to(self.device)\n",
    "#         weights = self.clustering_machine.sg_edge_weights[cluster].to(self.device)\n",
    "        macro_nodes = self.clustering_machine.sg_nodes[cluster].to(self.device)\n",
    "        test_nodes = self.clustering_machine.sg_test_target_nodes[cluster].to(self.device)\n",
    "        features = self.clustering_machine.sg_features[cluster].to(self.device)\n",
    "        target = torch.squeeze(self.clustering_machine.sg_test_targets[cluster].to(self.device), 1)\n",
    "#         prediction = self.model(edges, features, weights)\n",
    "        prediction = self.model(edges, features)\n",
    "        prediction = prediction[test_nodes,:]\n",
    "        return prediction, target\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Training a model.\n",
    "        \"\"\"\n",
    "        print(\"Training started.\\n\")\n",
    "        epochs = trange(self.args['epochs'], desc = \"Train Loss\")\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.args['lr_rate'])\n",
    "        self.model.train()\n",
    "        loss_values = list()\n",
    "        accuracy_values = list() ##### For Accuracy #####\n",
    "        for epoch in epochs:\n",
    "            random.shuffle(self.clustering_machine.clusters)\n",
    "            self.node_count_seen = 0\n",
    "            self.accumulated_training_loss = 0\n",
    "            for cluster in self.clustering_machine.clusters:\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "#                 batch_acc_score, batch_average_loss, node_count = self.do_forward_pass(cluster) ##### For Accuracy #####\n",
    "                batch_average_loss, node_count = self.do_forward_pass(cluster)\n",
    "                batch_average_loss.backward()\n",
    "                self.optimizer.step()\n",
    "                average_loss = self.update_average_loss(batch_average_loss, node_count)\n",
    "#                 batch_acc_score = self.update_average_loss(batch_acc_score, node_count) ##### For Accuracy #####\n",
    "            loss_values.append(average_loss)\n",
    "#             accuracy_values.append(batch_acc_score) ##### For Accuracy #####\n",
    "                # avg_acc_score = self.update_average_loss(batch_acc_score, node_count)\n",
    "            epochs.set_description(\"Train Loss: %g\" % round(average_loss,4))\n",
    "#             print(\"Accuracy-> \", batch_acc_score*100, \"%\") ##### For Accuracy #####\n",
    "#             epochs.set_description(\"Train Accuracy: \", avg_acc_score*100, \"%\") ##### For Accuracy Alternative #####\n",
    "            \n",
    "            \n",
    "        plt.figure(figsize=(10,5))\n",
    "        plt.title(\"Training Loss\", fontsize=18)\n",
    "#         plt.plot(accuracy_values, 'b', label='Train Accuracy')\n",
    "        plt.plot(loss_values, 'g', label=\"Train Loss\")\n",
    "        plt.xlabel(\"iterations\", fontsize=18)\n",
    "        plt.ylabel(\"Loss\", fontsize=18)\n",
    "        plt.legend(fontsize=18)\n",
    "        plt.xticks(fontsize=18)\n",
    "        plt.yticks(fontsize=18)\n",
    "        plt.show()\n",
    "        \n",
    "    def test(self):\n",
    "        \"\"\"\n",
    "        Scoring the test and printing the F-1 score.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        self.predictions = []\n",
    "        self.targets = []\n",
    "        for cluster in self.clustering_machine.clusters:\n",
    "            prediction, target = self.do_prediction(cluster)\n",
    "#             self.predictions.append(prediction.cpu().detach().numpy()) ##### For Classification #####\n",
    "            self.predictions.append(np.squeeze(prediction).cpu().detach().numpy()) ##### Changed for Regression #####\n",
    "            self.targets.append(target.cpu().detach().numpy())\n",
    "        self.targets = np.concatenate(self.targets)\n",
    "#         self.predictions = np.concatenate(self.predictions).argmax(1) ##### For Classification #####\n",
    "        self.predictions = np.concatenate(self.predictions) ##### Changed for Regression #####\n",
    "        print(len(self.targets),\" Type\", type(self.targets), \" ******** \",\" Type \", type(self.predictions), len(self.predictions))\n",
    "        print(\"R Squred-> \", r2_score(self.targets, self.predictions))\n",
    "        reg_Corr = pd.DataFrame({'Actual':self.targets, 'Predicted':self.predictions}) ##### Changed for Regression #####\n",
    "        reg_Corr.to_csv(\"/home/abose/HPC Analytics GCN/corr.csv\", sep=',') ##### Changed for Regression #####\n",
    "        print(\"Regression-> \", reg_Corr.corr(method='pearson')) ##### Changed for Regression #####\n",
    "#         test_acc_score= accuracy_score(self.targets, self.predictions) ##### For Classification #####\n",
    "#         ####F1_Score = f1_score(self.targets, self.predictions, average=\"micro\")\n",
    "#         ####F1_Score = f1_score(self.targets, self.predictions, pos_label=1, average = \"binary\")\n",
    "#         F1_Score = f1_score(self.targets, self.predictions, labels = [0]) ##### For Classification #####\n",
    "#         precision = precision_score(self.targets, self.predictions, labels = [0]) ##### For Classification #####\n",
    "#         recall = recall_score(self.targets, self.predictions, labels = [0]) ##### For Classification #####\n",
    "#         print(\"\\nTest Acc score: {:.4f}\".format(test_acc_score), \"  \\nPrecision score: {:.4f}\".format(precision), \"  \\nRecall score: {:.4f}\".format(recall), \"  \\nF-1 score: {:.4f}\".format(F1_Score)) ##### For Classification #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GCN part ends here ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Controller part ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1416,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_processor():\n",
    "    args=dict()\n",
    "    args['clustering_method']= \"No\"\n",
    "    args['epochs']= 50 ##### Changed for Regression #####\n",
    "#     args['epochs']= 400 ##### For Classification #####\n",
    "    args['seed']= 42\n",
    "    args['dropout']= 0.5\n",
    "#     args['lr_rate']= 0.01 ##### For Classification #####\n",
    "    args['lr_rate']= 0.2 ##### Changed for Regression #####\n",
    "    args['test_ratio']= 0.20\n",
    "    args['cluster_no']= 3\n",
    "#     args['layers']=[64, 64, 64] ##### For Classification #####\n",
    "    args['layers']=[64] ##### Changed for Regression #####\n",
    "    \n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1417,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_HPC_data(args):\n",
    "    nx_graph, node_neumeric_mapping, index_node_mapping = graph_generation()\n",
    "    load_Save_Target_Data(args)\n",
    "    \n",
    "    target = pd.read_csv('/home/abose/HPC Analytics GCN/targetDf.csv')\n",
    "    # nodeMap = pd.read_csv('/home/abose/HPC Analytics GCN/node_neumeric_mapping.csv')\n",
    "    target = target_mapper(target, node_neumeric_mapping, 0)\n",
    "    \n",
    "    train_Target = pd.read_csv('/home/abose/HPC Analytics GCN/train_TargetDf.csv')\n",
    "    # nodeMap = pd.read_csv('/home/abose/HPC Analytics GCN/node_neumeric_mapping.csv')\n",
    "    train_Target = target_mapper(train_Target, node_neumeric_mapping, 1)\n",
    "    \n",
    "    test_Target = pd.read_csv('/home/abose/HPC Analytics GCN/test_TargetDf.csv')\n",
    "    test_Target = target_mapper(test_Target, node_neumeric_mapping, 2)\n",
    "    \n",
    "    features = make_mapped_Embedding_for_Nodes(nx_graph, index_node_mapping)\n",
    "    return nx_graph, target, train_Target, test_Target, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1418,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph_DF DataType->  Src    object\n",
      "Des    object\n",
      "dtype: object\n",
      "status->  1  dum->  22829\n",
      "test_idxs->  [9624, 8493, 17398, 20955, 13173, 1845, 3434, 9032, 11524, 18054, 6793, 17778, 4738, 15660, 5856, 1846, 15366, 20087, 8982, 16456, 12680, 9289, 9853, 9318, 12206, 18696, 19449, 5449, 20570, 178, 21460, 17733, 3565, 4202, 16786, 8058, 7526, 21402, 19649, 22077, 11535, 18455, 5364, 13949, 6672, 6234, 767, 17576, 18424, 4098, 13626, 1889, 6295, 21842, 1662, 17502, 21934, 5484, 12784, 12636, 3248, 20124, 898, 14130, 205, 20746, 10370, 21193, 12527, 12744, 20721, 21964, 14037, 16289, 19200, 1761, 22828, 15019, 1792, 11890, 15544, 7219, 1281, 1340, 10429, 22121, 7086, 14169, 14861, 21767, 18023, 17021, 3860, 1894, 2529, 13061, 16773, 11145, 7402, 37, 10360, 7510, 915, 18630, 5456, 1503, 18580, 11579, 11288, 21818, 22734, 11576, 16874, 20074, 6068, 20865, 12178, 125, 5957, 1829, 1800, 14415, 1770, 21706, 11509, 7433, 17731, 16685, 20710, 20089, 2627, 1925, 10078, 3167, 2166, 18976, 2063, 17849, 10426, 10401, 17993, 3919, 4112, 287, 14024, 12099, 11691, 18149, 14445, 13257, 11541, 285, 19956, 14672, 3277, 20469, 12114, 13128, 19960, 18098, 1208, 17314, 21150, 18273, 12111, 16397, 18756, 17768, 22730, 20167, 1663, 6425, 21083, 11063, 4259, 16994, 7763, 5832, 7641, 5274, 21653, 17269, 16847, 22799, 5417, 9122, 5958, 17007, 14645, 2797, 17052, 6632, 9960, 10439, 11868, 11358, 10206, 16824, 12130, 5988, 11072, 13084, 15836, 11560, 1015, 7539, 7440, 5205, 15558, 4991, 3857, 22129, 17120, 9418, 15308, 19074, 22468, 10478, 2496, 2611, 11752, 1211, 15372, 10591, 798, 11737, 343, 18454, 1638, 11984, 16248, 230, 1122, 18164, 1724, 17656, 251, 11610, 8693, 3416, 7111, 4486, 14587, 9377, 6216, 8626, 10553, 22620, 17621, 17823, 15229, 7206, 3769, 4449, 7300, 6084, 12223, 18602, 19271, 10407, 16683, 12875, 107, 15891, 17706, 8697, 7479, 12674, 22203, 8130, 7243, 22105, 21502, 10691, 5894, 9436, 18010, 11665, 8816, 7556, 19323, 8797, 20725, 20436, 22622, 1117, 9177, 12324, 6742, 14474, 1827, 11623, 4735, 4042, 8122, 5461, 7710, 17111, 8098, 14553, 21088, 20230, 17866, 8296, 4323, 8927, 19893, 7009, 10813, 14920, 21834, 10300, 3830, 9512, 6148, 15852, 9529, 16363, 9343, 2877, 18334, 11766, 13174, 19128, 13478, 4078, 8594, 9795, 6796, 492, 4933, 6496, 10182, 8926, 3377, 14968, 6637, 19411, 21455, 75, 17795, 17573, 4350, 6572, 5642, 22247, 19605, 12740, 3461, 9264, 1613, 1311, 17485, 7629, 20876, 11639, 5375, 20402, 7649, 17177, 15810, 8552, 16619, 13141, 12252, 6190, 16882, 6045, 22253, 20439, 15888, 17711, 19170, 13973, 18989, 5597, 4647, 12694, 18288, 9496, 4839, 8664, 17490, 16626, 10148, 2187, 6463, 15374, 7709, 5336, 20804, 5175, 22273, 19119, 1791, 16174, 19182, 4831, 19444, 13663, 14820, 14648, 7509, 17748, 7057, 21387, 22412, 13504, 15287, 14633, 14607, 22533, 2274, 6523, 15369, 6281, 8662, 13365, 39, 4062, 806, 18073, 22783, 8748, 19624, 19378, 7346, 9081, 14067, 1815, 15958, 14663, 11216, 15609, 6864, 7151, 2239, 13954, 7307, 4569, 17672, 2229, 7996, 7551, 8402, 22040, 12418, 15831, 7337, 20532, 7606, 9723, 2120, 2334, 17049, 3443, 13743, 3478, 16004, 17782, 11569, 7287, 6118, 5269, 5004, 6217, 10369, 10521, 10922, 1912, 1513, 2981, 5622, 21860, 17099, 22432, 4760, 10295, 10838, 4894, 12079, 8683, 616, 10973, 3208, 17357, 11813, 19592, 10425, 22310, 12606, 4373, 10897, 3677, 19549, 22180, 8564, 21113, 17968, 7491, 21805, 13689, 21091, 11354, 3155, 13814, 18775, 4742, 4914, 967, 7478, 553, 22340, 12184, 12377, 16099, 19133, 2438, 4640, 8128, 22059, 16800, 9677, 1866, 8042, 19150, 1716, 1191, 21293, 16954, 17453, 10840, 8465, 12281, 22670, 9977, 12517, 1888, 6094, 9669, 9813, 20189, 3856, 8496, 3406, 16677, 5693, 3675, 3495, 19860, 2632, 7634, 19772, 12711, 15792, 1427, 20309, 16838, 10898, 7878, 1947, 7639, 3643, 3525, 8746, 18710, 20822, 7777, 9798, 18406, 14327, 14134, 14791, 19724, 16327, 14074, 1258, 14755, 5408, 462, 9717, 4513, 13829, 10315, 7785, 6500, 20350, 13001, 640, 1936, 18228, 16065, 12009, 19300, 8403, 3341, 9647, 197, 5074, 739, 10913, 3247, 1006, 7760, 7319, 16532, 15030, 6106, 1387, 20419, 12143, 6856, 22080, 22404, 20129, 14447, 9965, 11278, 5133, 20039, 21034, 1450, 11020, 15793, 2133, 19320, 8995, 8919, 9434, 7849, 16114, 21053, 15208, 22687, 19878, 1104, 6355, 13734, 5466, 5907, 19715, 15874, 1980, 13816, 7796, 14103, 5508, 3135, 11195, 12876, 17234, 12055, 11289, 3574, 20045, 12333, 13147, 12812, 15755, 11362, 6322, 19966, 4241, 4770, 14058, 682, 7966, 8749, 18610, 1084, 21297, 8350, 15855, 14748, 22486, 19940, 10099, 7592, 7236, 6669, 20908, 8420, 3518, 399, 53, 16449, 16895, 20750, 22126, 5131, 15721, 19530, 9892, 1931, 18579, 4664, 14139, 12881, 6301, 17655, 3802, 4594, 8263, 20527, 17666, 5483, 11117, 9748, 11946, 7978, 6747, 3489, 5667, 10497, 18975, 19778, 8383, 10766, 3086, 5283, 7636, 3744, 20797, 1164, 16776, 9235, 8868, 5052, 11083, 9902, 22403, 19626, 17293, 14735, 7808, 19647, 13640, 19947, 6384, 10982, 16782, 10602, 7314, 18712, 18481, 20217, 8699, 20216, 4289, 7830, 1301, 21661, 15778, 9266, 2435, 14596, 15884, 21092, 13815, 1806, 17764, 16372, 11090, 1082, 21658, 6015, 17563, 11300, 1929, 4728, 7959, 1331, 13886, 1284, 14794, 2235, 15469, 20958, 18119, 21530, 18282, 11909, 5722, 10067, 12766, 20274, 5523, 772, 5272, 18940, 8501, 7454, 21667, 20269, 5235, 7664, 10815, 19754, 11657, 7431, 4858, 16526, 14845, 2520, 6593, 13305, 9952, 17526, 3959, 7688, 20938, 12073, 12191, 4816, 22336, 21249, 13686, 12140, 13660, 7051, 2715, 5573, 21975, 16689, 2737, 17844, 16602, 22607, 18038, 11878, 552, 2934, 11186, 7316, 1227, 22669, 3283, 14956, 4027, 7646, 10054, 13029, 19814, 2230, 13940, 1081, 13800, 7953, 1358, 9994, 14122, 11992, 16173, 8351, 4181, 9648, 851, 1096, 2308, 922, 10902, 6152, 7290, 22252, 109, 838, 22321, 1518, 13767, 20077, 7783, 17057, 18020, 18933, 2642, 18364, 3146, 8518, 19228, 1891, 2799, 4321, 22011, 18865, 3347, 4054, 8276, 14490, 547, 20818, 1559, 9526, 11496, 9006, 9806, 3969, 18603, 17707, 5678, 12678, 13286, 12971, 17233, 364, 19661, 7532, 20763, 12912, 12329, 13729, 2744, 17880, 9845, 3619, 2408, 10635, 13782, 12777, 16280, 14300, 9523, 2659, 90, 12292, 11244, 8076, 7628, 19736, 1382, 10499, 17444, 22703, 1230, 17419, 15327, 13930, 1103, 2801, 1738, 6123, 4677, 3893, 3648, 14047, 584, 9452, 10397, 12416, 5479, 17321, 6193, 10440, 8924, 5998, 21234, 14768, 9614, 9084, 5231, 20615, 17046, 3500, 16400, 6205, 2053, 9574, 12787, 22298, 15680, 14600, 11819, 16747, 15786, 19805, 13085, 1129, 9898, 3615, 14010, 11368, 22209, 6601, 10335, 15222, 5931, 11627, 658, 544, 2873, 4189, 8578, 18734, 12978, 21116, 6725, 9054, 9256, 8249, 17651, 10405, 18513, 9322, 3298, 6583, 10279, 20026, 16260, 8984, 22573, 21772, 16601, 8682, 7461, 1389, 6959, 11896, 8088, 9058, 15412, 21033, 6760, 12536, 14438, 20301, 18078, 3164, 18600, 15752, 5317, 4981, 18204, 655, 3325, 18556, 7142, 14243, 13245, 1644, 20831, 16550, 11337, 455, 12878, 13702, 3166, 14254, 10308, 21636, 1350, 4280, 1841, 18018, 14237, 19938, 14137, 5542, 3082, 12340, 14158, 8711, 4810, 21250, 10632, 4918, 2761, 7494, 22657, 20700, 20363, 21977, 1646, 5629, 16442, 8222, 8472, 7659, 29, 2619, 18637, 16763, 5812, 14055, 13718, 20119, 20146, 18453, 1599, 18396, 5814, 21028, 19297, 15830, 11155, 11840, 3642, 10329, 19927, 16854, 1870, 6284, 16758, 6990, 15517, 11128, 3235, 9542, 5753, 2674, 16542, 5372, 22206, 19490, 2231, 20775, 10585, 15838, 15344, 8973, 8577, 9077, 2656, 10249, 22218, 1808, 7568, 22605, 1621, 17165, 19143, 3705, 21047, 19755, 4253, 15458, 11767, 14964, 9162, 15443, 3590, 8354, 8481, 6562, 9378, 2767, 19598, 10938, 22812, 6274, 12649, 22143, 4031, 8022, 15170, 10850, 9223, 19856, 6270, 8997, 1813, 8003, 4669, 1464, 12374, 6096, 15064, 2024, 2803, 9113, 21707, 14402, 19603, 7682, 15980, 10976, 11332, 15909, 11379, 17759, 517, 6032, 71, 12465, 13658, 7013, 5472, 10294, 116, 16441, 20575, 11483, 14469, 651, 5049, 1512, 6651, 10983, 14296, 14306, 12494, 3956, 18581, 14787, 11109, 405, 19251, 10614, 22788, 19313, 12053, 20953, 11051, 4590, 19658, 6677, 9707, 4036, 263, 3352, 19872, 18670, 20361, 10423, 14573, 6802, 2565, 8126, 4955, 15017, 16401, 13592, 17296, 18514, 9712, 6980, 18891, 18729, 11777, 7324, 22585, 12986, 7963, 4411, 7954, 7096, 1430, 12921, 14093, 6599, 3390, 3268, 13555, 2900, 4405, 16698, 20299, 9205, 6730, 1881, 20883, 8298, 15085, 5356, 2819, 1496, 5668, 1018, 19588, 10443, 12027, 14568, 2049, 13819, 17407, 17760, 8227, 4790, 3842, 21324, 1713, 22485, 18763, 2329, 8935, 20982, 20401, 17358, 4015, 22300, 10490, 18428, 14574, 7356, 13309, 3898, 15828, 19761, 4537, 19691, 13328, 7161, 8900, 15013, 9616, 16755, 14582, 13157, 3718, 12577, 12189, 21752, 14769, 18329, 25, 8233, 15932, 21029, 22615, 16003, 9050, 8489, 11527, 3323, 7355, 2051, 5772, 20549, 2542, 10193, 1048, 19440, 19171, 594, 16544, 17458, 4739, 18854, 8171, 16859, 3853, 418, 15934, 2697, 11603, 1016, 21676, 18191, 14810, 19606, 16856, 16425, 17258, 21132, 6854, 19909, 13895, 15869, 11187, 8814, 22714, 19749, 8792, 20919, 19925, 591, 2287, 18998, 16236, 3735, 3419, 22567, 19883, 13723, 5079, 18248, 5308, 20456, 12710, 15095, 6652, 8150, 22057, 18673, 6188, 13818, 17528, 15975, 428, 7294, 603, 11255, 15712, 4410, 3835, 15298, 1049, 17755, 17529, 18422, 6017, 17801, 5519, 8692, 17139, 19785, 2288, 3878, 10847, 12797, 2451, 5759, 13041, 19492, 11403, 12195, 542, 5382, 1091, 2249, 22593, 5330, 8497, 22149, 22358, 15045, 18997, 5126, 19525, 5611, 15428, 4415, 16570, 4966, 2510, 16359, 9572, 13191, 3571, 2690, 17540, 17832, 9762, 3649, 22436, 13331, 18399, 10082, 22255, 11129, 12574, 18639, 21668, 5745, 16407, 13842, 15658, 13244, 19628, 8812, 12541, 18674, 14736, 15896, 6895, 15075, 13808, 12856, 2098, 10367, 21876, 14179, 1817, 12788, 5193, 10421, 5102, 13462, 575, 3178, 15406, 10191, 16831, 12210, 2544, 4304, 22235, 12405, 388, 17100, 9464, 18012, 587, 11479, 110, 4088, 19742, 21214, 7296, 3252, 4379, 20934, 21974, 10408, 7627, 17663, 6690, 15031, 6544, 19496, 9941, 18352, 3844, 17943, 2781, 6552, 20203, 21302, 2089, 81, 7541, 19273, 20064, 2277, 8170, 6912, 5386, 2718, 21347, 4143, 22137, 6811, 13190, 13573, 4314, 909, 13681, 12214, 3637, 13275, 5912, 21055, 18835, 20757, 2705, 14341, 14353, 17351, 5589, 4217, 20557, 10278, 20660, 752, 18593, 12803, 10412, 6741, 17710, 22550, 15985, 2625, 8191, 6653, 2007, 10710, 9026, 4116, 406, 20606, 15913, 2800, 3514, 11353, 12935, 19071, 5041, 7992, 21449, 836, 16220, 13719, 13911, 13820, 3622, 6621, 20011, 10848, 18000, 18211, 733, 12408, 18886, 5867, 17174, 22806, 12928, 7713, 12570, 2209, 5174, 21786, 277, 19367, 6332, 12472, 20542, 22015, 2753, 22241, 22130, 18170, 18475, 18538, 16146, 8685, 4129, 1121, 12882, 3422, 7662, 6020, 19903, 6568, 16716, 15173, 11721, 15889, 19095, 18214, 961, 11949, 20389, 385, 4003, 6783, 15753, 4335, 19770, 1960, 17058, 2826, 6870, 21578, 15301, 11120, 8287, 16042, 19545, 15685, 9052, 14867, 9364, 21588, 21535, 16863, 15807, 5159, 19155, 4019, 19364, 6785, 2321, 11321, 6486, 17990, 20446, 12825, 6761, 2888, 18309, 5082, 7895, 3631, 21219, 18450, 6426, 5315, 16005, 6706, 4277, 21686, 2306, 10714, 22544, 11024, 21040, 1290, 7695, 18725, 17618, 1140, 470, 18863, 10020, 18128, 16774, 16189, 3101, 9188, 15535, 4281, 14960, 4227, 11633, 12708, 12729, 17690, 10197, 4125, 16553, 8005, 18954, 35, 4917, 11085, 19625, 17115, 20306, 10245, 17310, 22238, 9483, 2815, 18961, 14498, 8562, 11574, 21529, 13745, 5347, 17719, 17593, 20196, 14395, 12748, 4589, 7603, 7166, 19068, 10337, 7810, 5915, 2396, 14884, 3026, 20661, 12083, 19445, 20004, 18754, 21198, 11860, 12212, 379, 18586, 15667, 16424, 14050, 20743, 1920, 9057, 4864, 6305, 16591, 19216, 13526, 4179, 761, 19077, 10674, 20839, 3320, 7968, 3192, 22663, 20520, 14477, 18069, 8933, 18276, 6285, 2777, 3044, 4083, 18888, 17029, 20879, 3866, 964, 15394, 6167, 20170, 19807, 21420, 17790, 5886, 2952, 8895, 12686, 18147, 12279, 17956, 12758, 1679, 18255, 5267, 22704, 2545, 17875, 3927, 9277, 4531, 40, 12547, 21946, 6427, 20756, 2590, 11047, 7375, 14994, 5157, 11231, 16097, 18321, 12933, 13854, 12669, 13113, 10301, 1471, 14478, 1186, 4229, 1472, 19048, 18182, 16045, 14287, 7380, 7147, 11815, 21074, 15112, 21273, 16717, 21547, 11312, 14433, 4480, 4346, 21175, 14344, 10990, 12525, 13263, 211, 14813, 5758, 11301, 10393, 12799, 3568, 1415, 1728, 12391, 12015, 21564, 10081, 15348, 16213, 12705, 6256, 4687, 18817, 419, 19427, 10231, 18232, 7269, 14003, 2372, 4428, 8364, 7751, 9146, 18517, 887, 5552, 22223, 7986, 21553, 20383, 936, 11241, 3140, 299, 8773, 8109, 3700, 11870, 17221, 2708, 21510, 16271, 8538, 7416, 2904, 2961, 14723, 13200, 9380, 11596, 5810, 11492, 5630, 1132, 21284, 14006, 15489, 1278, 5509, 13117, 3953, 2358, 10387, 21271, 9426, 12431, 21675, 21737, 11982, 4603, 11163, 5751, 3524, 22771, 6767, 15395, 4004, 8844, 4188, 15596, 1535, 15587, 1589, 10743, 21506, 22372, 7041, 13987, 2399, 9365, 5559, 21211, 9942, 18904, 6195, 17291, 446, 14586, 15702, 20966, 2224, 21594, 868, 9834, 21419, 14781, 14577, 1107, 6209, 19790, 18177, 473, 4585, 3576, 14847, 3929, 16075, 4197, 6838, 5701, 17290, 15930, 5358, 659, 18626, 2576, 15653, 4488, 13807, 19275, 6207, 20809, 506, 9984, 21358, 2165, 22511, 15262, 20902, 18818, 7566, 11497, 14655, 21699, 2644, 13952, 22450, 9686, 7466, 11803, 17911, 13253, 9027, 1292, 6067, 12509, 4592, 8487, 3673, 17570, 2986, 8531, 9409, 19665, 5582, 166, 13216, 11601, 16537, 5639, 12499, 18109, 6557, 5971, 21216, 6469, 546, 14652, 13223, 14298, 2294, 4347, 19239, 11908, 17773, 14871, 5658, 7955, 21264, 22037, 16735, 15279, 8953, 5459, 15744, 4976, 19841, 19417, 15118, 13314, 12623, 22563, 5697, 20013, 19438, 14097, 703, 21251, 14147, 1505, 9604, 3554, 22087, 17797, 20722, 578, 20289, 302, 4524, 20830, 10305, 2149, 21310, 4959, 14680, 11877, 3332, 10970, 4657, 14763, 2902, 18090, 17995, 327, 22549, 20925, 15916, 18408, 1797, 18776, 16256, 4553, 9051, 12915, 9629, 11136, 3305, 17957, 12044, 14365, 17835, 7292, 4773, 9036, 1159, 15700, 16058, 6779, 8010, 14095, 12977, 13289, 9216, 15481, 12970, 20376, 10613, 21745, 105, 15908, 623, 13048, 22196, 17510, 22117, 13359, 670, 5554, 8208, 3059, 16049, 17901, 22441, 2344, 18690, 5560, 14351, 7090, 12151, 12386, 9801, 19775, 204, 13884, 11001, 2282, 16849, 12271, 10199, 9454, 12715, 6078, 5028, 13931, 5935, 22793, 2789, 17550, 395, 12227, 3551, 3614, 12070, 13336, 11208, 7220, 20090, 3726, 5416, 6888, 16704, 5254, 19636, 3889, 8721, 18678, 22556, 2748, 5091, 22312, 16908, 12085, 11413, 16561, 14189, 7784, 13593, 7138, 22695, 12196, 21195, 21582, 10404, 871, 8359, 21112, 18846, 2976, 13419, 3024, 20957, 20349, 893, 2513, 17856, 9747, 4294, 7460, 15675, 3156, 20814, 8889, 17315, 6280, 22268, 12958, 6269, 3709, 13620, 18910, 14403, 16996, 18458, 2211, 8817, 10843, 10568, 17472, 386, 20253, 5320, 13116, 13969, 21554, 7020, 14151, 7767, 12460, 17414, 881, 12884, 17739, 11808, 6848, 11341, 20324, 17223, 12551, 22785, 2652, 741, 16980, 2137, 12555, 15968, 2559, 3202, 9964, 13841, 12588, 13303, 5385, 1952, 3965, 17173, 1086, 13980, 6248, 16180, 10428, 21154, 16057, 11530, 20224, 15399, 7554, 8319, 7699, 3739, 2867, 3696, 12425, 14886, 11271, 11219, 19419, 122, 10772, 21815, 14656, 5162, 17908, 4496, 2949, 12782, 15875, 3397, 13270, 22520, 13690, 16138, 18759, 10282, 19944, 3818, 1231, 6511, 17032, 21655, 22649, 6577, 8704, 11419, 14557, 10969, 10940, 21009, 3743, 5788, 17083, 1352, 9142, 9236, 14751, 461, 22339, 21736, 10741, 9982, 7385, 11196, 9623, 17555, 10814, 4731, 19193, 10302, 167, 10871, 4515, 4927, 19948, 19540, 17144, 338, 11993, 15950, 11008, 6382, 13057, 7064, 5178, 18086, 16009, 21181, 1735, 16966, 4220, 17080, 10858, 2743, 10648, 7386, 7738, 6458, 8323, 17826, 2956, 9660, 6319, 6072, 3249, 17265, 15858, 7136, 436, 18669, 15437, 914, 28, 11893, 8105, 12934, 16463, 13454, 6420, 10736, 7615, 17217, 1574, 19821, 20534, 11526, 14368, 2508, 11931, 10681, 13761, 16650, 4800, 19131, 18895, 6372, 8459, 20875, 2573, 12060, 7721, 4505, 13874, 16748, 8461, 1251, 10608, 11035, 17380, 4039, 20851, 15582, 2723, 21756, 11054, 6246, 10327, 16163, 16999, 1604, 2552, 3454, 2739, 9601, 1972, 7116, 15717, 3388, 14780, 17473, 18277, 4214, 12036, 18872, 2220, 20340, 18143, 3015, 5687, 18848, 20959, 12983, 13021, 19954, 12345, 11923, 4096, 10971, 8445, 13340, 9274, 10685, 22539, 11624, 7203, 12459, 822, 18349, 6506, 12173, 6732, 21669, 17480, 735, 3223, 15862, 20464, 1123, 1901, 11394, 11645, 6769, 19088, 7495, 83, 15559, 8161, 22315, 18159, 18796, 3271, 13913, 5579, 3079, 12047, 14168, 17178, 10060, 1297, 22577, 19631, 19998, 9506, 20555, 21416, 9101, 11294, 11034, 16308, 8691, 8262, 4061, 7125, 117, 9665, 20677, 1706, 17268, 18599, 9799, 18133, 13024, 6471, 757, 17170, 5918, 13496, 9972, 10007, 7349, 7186, 6338, 18132, 22480, 18462, 8160, 778, 19328, 21493, 7787, 3139, 21285, 11056, 18477, 2252, 16898, 4454, 11100, 13053, 5942, 11670, 10288, 9359, 7008, 13456, 20072, 1985, 11696, 9917, 4389, 12870, 6351, 18733, 7317, 5731, 6778, 17051, 9457, 14979, 14418, 5195, 1736, 7854, 2631, 8246, 10281, 13433, 9833, 5242, 19335, 22136, 17713, 3876, 10890, 2297, 16028, 6169, 9693, 15140, 8607, 18531, 20391, 8017, 1669, 6312, 9937, 22408, 15049, 10173, 7208, 5940, 329, 5280, 16518, 16974, 1790, 1823, 6133, 19307, 9268, 17903, 7864, 2941, 13609, 6890, 1133, 13879, 9631, 5581, 4602, 18445, 20650, 13982, 13330, 4827, 2504, 15846, 3724, 11936, 903, 1867, 4781, 11116, 17015, 906, 15523, 7114, 1997, 11644, 7150, 13812, 4733, 7909, 9014, 13680, 19138, 20098, 9907, 5184, 595, 22608, 14045, 4249, 2150, 22490, 11675, 16778, 619, 8080, 1414, 16511, 8881, 2774, 22047, 15167, 15850, 4211, 8094, 16055, 19384, 12124, 3029, 5050, 9859, 7866, 12795, 6224, 18009, 5186, 5877, 5859, 6238, 13246, 300, 19379, 5183, 4660, 14980, 5012, 14335, 16316, 18829, 14279, 15771, 6831, 17149, 10031, 3570, 11517, 18366, 4520, 8266, 22648, 11857, 13493, 2936, 19293, 235, 14998, 18900, 20864, 13637, 11539, 330, 10716, 10757, 21955, 2311, 21503, 5489, 22350, 466, 3758, 3685, 13240, 19722, 3938, 4929, 12700, 19623, 14579, 22508, 3035, 13375, 14033, 21247, 22564, 64, 14212, 6509, 17010, 249, 3821, 19670, 6353, 9656, 7450, 20257, 8629, 1765, 16844, 5663, 17128, 11501, 13357, 6842, 11787, 12019, 7170, 6733, 16426, 2092, 10441, 15982, 20719, 3364, 19350, 10619, 6183, 21901, 17326, 434, 6489, 2540, 7467, 10275, 10665, 14955, 18036, 22246, 15806, 22291, 18575, 3609, 20454, 3779, 10502, 20385, 21525, 2348, 3468, 11712, 8781, 20495, 15396, 20240, 20093, 5981, 2691, 6792, 17286, 5106, 15434, 14996, 22016, 5219, 1760, 13748, 7254, 15725, 7563, 2741, 13415, 20445, 19621, 21972, 3023, 2177, 2982, 5651, 6112, 22543, 21866, 14610, 7588, 14394, 14396, 6119, 22060, 22289, 15612, 15812, 18511, 3684, 22038, 21415, 22616, 4171, 15917, 3098, 16901, 9970, 15613, 17542, 10625, 4862, 6926, 5763, 6916, 2881, 10176, 2966, 1776, 2538, 5142, 13120, 9875, 5443, 1399, 13531, 2589, 21145, 17402, 18716, 1158, 93, 6380, 18762, 12501, 7352, 5458, 17636, 9347, 18564, 5475, 19765, 5525, 7394, 15353, 14654, 14842, 18884, 14938, 9233, 1455, 7404, 16540, 22761, 19886, 5140, 22668, 3904, 12637, 12423, 4049, 1995, 11681, 5374, 15407, 16275, 21288, 16923, 14262, 11443, 5244, 13248, 13075, 14331, 11490, 13396, 13896, 19147, 3902, 4324, 6010, 20457, 3880, 2040, 10328, 18072, 9923, 20085, 7693, 5995, 21300, 14559, 11700, 12726, 12498, 16618, 1249, 686, 9664, 14174, 6242, 2609, 12567, 6782, 20596, 6887, 16546, 131, 20001, 8073, 8506, 9585, 14236, 5122, 5161, 21306, 6745, 2802, 7663, 5735, 14011, 2081, 11267, 2719, 20619, 15892, 9180, 6634, 12991, 14860, 12622, 8488, 17135, 16736, 21682, 15993, 4668, 8071, 12284, 947, 5624, 22081, 8388, 9291, 108, 3790, 15174, 6213, 1970, 8059, 5522, 337, 7919, 2946, 16646, 12152, 20082, 15191, 21877, 4064, 14239, 21753, 6233, 8771, 20314, 7881, 6452, 16186, 14376, 5944, 18409, 5060, 6367, 2983, 9943, 22396, 494, 10265, 17969, 10248, 19815, 14782, 9154, 736, 18056, 15266, 294, 16483, 19270, 11328, 17212, 10704, 3103, 19916, 12576, 463, 3562, 22416, 6560, 4598, 1022, 10775, 22084, 8739, 20936, 11956, 5528, 7217, 15028, 21808, 14167, 1360, 7132, 15967, 21361, 1310, 15656, 1694, 14767, 19334, 8340, 2227, 1342, 3348, 4699, 8708, 819, 3760, 11948, 10833, 7058, 14250, 19819, 13468, 6021, 1065, 2303, 13108, 9950, 10907, 4661, 17565, 19085, 3848, 4522, 16718, 13605, 10382, 22172, 14035, 3132, 4644, 11045, 16733, 14213, 15737, 9876, 2106, 10319, 4909, 3189, 12638, 18179, 16630, 2762, 15991, 19877, 16611, 3050, 12762, 3100, 296, 17354, 797, 986, 5700, 20986, 4769, 14375, 6309, 9527, 14599, 7442, 17783, 10781, 21454, 4548, 22192, 6291, 2295, 17336, 2566, 7922, 17230, 17810, 17699, 10471, 19370, 13984, 16508, 21014, 10962, 3703, 20421, 10896, 84, 18407, 7357, 16589, 1408, 13134, 7548, 22023, 12546, 10323, 12259, 13093, 2056, 17524, 611, 488, 6860, 17483, 12301, 15083, 2791, 2846, 14971, 16656, 4295, 10194, 18495, 12456, 15780, 8860, 1840, 13792, 22179, 2046, 1004, 14404, 20869, 2380, 16727, 15743, 6889, 10103, 840, 13232, 12254, 20564, 3520, 7211, 2327, 11410, 3663, 2022, 912, 17447, 5840, 22470, 22155, 22800, 11480, 13774, 21746, 19316, 17816, 20942, 15859, 10963, 9055, 3800, 12056, 8782, 6141, 12939, 13705, 11190, 17854, 2164, 9438, 15378, 5066, 4499, 18160, 3606, 18618, 22265, 9474, 479, 7329, 2539, 7061, 13697, 2933, 17450, 22684, 5669, 21408, 11519, 8830, 3466, 4518, 14414, 8965, 16739, 15801, 18919, 16837, 5109, 21798, 9515, 18558, 2087, 21622, 3197, 21562, 14260, 7749, 18065, 16446, 10735, 22481, 19282, 3077, 12260, 5437, 177, 18572, 7523, 2491, 7643, 19579, 16461, 2572, 21220, 14340, 16416, 5924, 19996, 13352, 9104, 13450, 1126, 12943, 2813, 11143, 11585, 20514, 5451, 17210, 20684, 6525, 6439, 16078, 1420, 11373, 4296, 720, 6313, 2828, 20121, 22176, 9043, 16187, 5398, 8324, 6880, 3529, 15183, 9100, 10790, 19810, 3431, 16579, 11950, 21681, 21733, 22000, 10572, 19141, 13910, 6466, 7967, 22186, 13902, 20794, 21500, 2305, 417, 1987, 18044, 12086, 19326, 9370, 20829, 12604, 6064, 14444, 21228, 17585, 14206, 437, 4046, 14906, 21143, 6661, 6343, 4508, 3188, 3960, 12255, 7782, 19355, 10318, 181, 7765, 15887, 15179, 6979, 3199, 21146, 7196, 22282, 22115, 17239, 13019, 7918, 20731, 21861, 5672, 3585, 22031, 13560, 18358, 13916, 557, 18296, 495, 841, 4627, 6919, 629, 17137, 14525, 16016, 1733, 16070, 4306, 70, 22046, 6137, 9046, 1380, 18949, 8705, 2524, 19116, 20268, 7067, 18032, 4915, 2428, 14104, 18274, 5096, 19581, 5864, 21367, 3272, 5067, 20095, 11464, 7524, 13225, 8205, 13105, 16939, 9911, 375, 14730, 19558, 20800, 17850, 324, 1021, 13373, 5594, 7856, 17696, 21678, 21479, 2490, 17206, 17732, 14693, 17934, 5900, 1831, 15823, 259, 4561, 6999, 3997, 5620, 21174, 11159, 18640, 12470, 17394, 5660, 8946, 965, 22163, 22818, 17327, 22346, 1883, 21187, 5344, 3401, 9736, 19863, 4953, 10774, 12826, 18016, 2175, 6565, 18622, 13900, 6143, 12725, 20923, 22592, 17160, 392, 15620, 614, 12375, 4369, 414, 6824, 14537, 20158, 15633, 22012, 11082, 14829, 9650, 9800, 17986, 10271, 18548, 21854, 17101, 20008, 21885, 402, 5007, 11452, 17138, 7328, 3121, 14424, 7311, 10960, 7129, 7851, 12993, 4021, 6680, 17302, 9249, 15808, 22530, 12595, 3265, 6967, 22656, 6752, 22188, 17484, 20545, 12466, 22624, 9865, 12078, 4180, 21935, 5377, 11203, 1174, 19432, 13169, 7927, 2136, 19096, 5118, 704, 15312, 20853, 7137, 12349, 2669, 8404, 12201, 13598, 4325, 1418, 16860, 19439, 2461, 14088, 1597, 860, 9022, 7122, 6719, 17642, 18442, 22360, 22672, 9029, 13341, 9930, 5009, 11184, 22254, 3034, 10788, 10208, 17944, 6107, 17608, 16380, 13394, 2474, 3983, 12721, 15949, 7596, 19792, 17612, 15101, 2514, 22559, 3794, 1137, 11620, 14569, 16109, 15086, 21235, 6488, 20344, 3172, 13010, 1404, 10331, 8998, 13983, 21331, 5989, 21095, 15698, 7489, 5643, 22766, 22553, 9520, 12275, 8421, 15203, 19304, 3021, 6187, 5727, 1115, 19097, 20685, 2756, 4700, 8569, 7974, 20810, 14852, 19486, 4368, 8916, 22826, 18045, 16633, 10230, 16068, 5136, 16883, 21471, 5013, 22644, 18398, 10812, 18140, 22679, 10483, 3915, 3515, 3091, 16475, 9918, 20057, 17705, 8252, 20841, 12503, 19900, 11414, 5980, 12954, 17190, 10616, 12658, 9956, 5463, 13825, 20704, 16728, 22382, 10728, 1254, 14689, 18024, 16468, 16462, 20782, 2703, 19684, 18497, 8256, 9028, 5862, 12722, 17253, 4543, 20567, 16225, 20056, 16214, 21312, 16949, 17487, 1325, 15093, 14015, 13786, 4752, 21376, 3948, 17888, 21458, 3792, 18423, 9276, 7198, 16033, 10770, 9587, 1702, 8901, 6927, 2347, 13557, 867, 4923, 14422, 8601, 6480, 16899, 16379, 4557, 16699, 3538, 16620, 2704, 5633, 18235, 3841, 7579, 799, 10688, 4037, 1273, 14543, 22034, 3607, 8539, 20565, 596, 10214, 5250, 9467, 6404, 22198, 15750, 10915, 10224, 328, 16428, 9119, 14437, 14749, 13923, 3776, 19864, 22630, 6339, 21744, 8803, 19713, 18642, 21673, 17420, 169, 2055, 16069, 16757, 6554, 394, 1214, 5390, 3808, 20787, 7859, 8511, 17146, 3840, 2391, 3000, 19766, 620, 6395, 3905, 16282, 2008, 4330, 15761, 18898, 13769, 20702, 12747, 19185, 146, 11215, 6004, 10968, 5666, 13170, 1052, 4210, 5110, 2292, 14040, 1721, 6996, 19743, 16720, 10145, 17767, 19446, 11844, 21859, 13649, 10411, 12642, 8426, 14992, 22108, 20595, 5031, 7267, 7455, 8658, 3358, 19038, 18208, 18083, 1225, 22181, 10729, 19789, 13299, 22044, 17224, 13467, 16453, 6100, 17750, 10309, 453, 12211, 11151, 19653, 12592, 10935, 11570, 8331, 14575, 15285, 3871, 729, 7056, 20724, 9914, 20138, 18129, 19281, 12013, 1023, 15113, 13522, 13731, 3095, 9088, 19164, 5934, 104, 19689, 12495, 12295, 2475, 11420, 4901, 7303, 7322, 769, 12625, 16586, 242, 14145, 8455, 1067, 22820, 12302, 5241, 4463, 20430, 11053, 4967, 12387, 17001, 11260, 9095, 7237, 15328, 10136, 4595, 10472, 16512, 19690, 489, 16641, 2657, 8305, 10777, 21795, 4803, 11225, 5246, 164, 5355, 1220, 786, 9076, 21824, 17078, 14526, 5046, 11204, 564, 16578, 17374, 7482, 11493, 20303, 6250, 14846, 14605, 11776, 8044, 1432, 4921, 1226, 20603, 20776, 8186, 5083, 7019, 6092, 2240, 12616, 10531, 8890, 8358, 11256, 3873, 3123, 8530, 19651, 16959, 6755, 14277, 6024, 12999, 14481, 1033, 13671, 20913, 9864, 4535, 19706, 12538, 17994, 1873, 13787, 13821, 8162, 12323, 18859, 7315, 12659, 7611, 19094, 20689, 18845, 17900, 21155, 11708, 22328, 4394, 19393, 7275, 3814, 19244, 7152, 1977, 16491, 18913, 17610, 15971, 11158, 7297, 12500, 17493, 20134, 7651, 8182, 10207, 925, 19111, 13198, 9160, 11636, 20358, 12976, 18870, 21232, 6675, 4609, 6619, 16525, 2026, 5983, 10816, 15359, 2366, 2330, 17996, 18439, 18883, 14592, 11364, 5975, 22617, 17141, 11400, 449, 2285, 5738, 9618, 6884, 6681, 7410, 20016, 12849, 7914, 8936, 2759, 20285, 1862, 11344, 12258, 3826, 4840, 12858, 10779, 6194, 14563, 1367, 13766, 17226, 9296, 4355, 8433, 4126, 9431, 14463, 1687, 3452, 1617, 2754, 21886, 19832, 9867, 4503, 2597, 10582, 4287, 5545, 10680, 19315, 10744, 9316, 20122, 3237, 7010, 18405, 8329, 17776, 6896, 16395, 21804, 10212, 10368, 15631, 483, 1272, 14512, 20191, 17181, 17738, 11113, 20440, 5671, 5034, 2711, 21968, 897, 13026, 12125, 7462, 15500, 112, 18518, 18178, 15618, 9210, 4218, 12037, 13510, 12228, 1289, 8533, 13181, 22733, 22805, 2226, 3264, 12215, 849, 3213, 4881, 17195, 2384, 21781, 4187, 20432, 13039, 6082, 19662, 21952, 15495, 9841, 3028, 12568, 22476, 12341, 9739, 9280, 6921, 19475, 16805, 15379, 333, 22116, 10586, 8636, 4153, 16520, 7944, 17709, 9560, 13086, 6397, 9668, 7792, 20113, 1750, 4993, 12790, 15751, 16194, 6080, 14558, 16027, 2954, 20273, 8600, 21031, 17227, 11445, 8389, 4726, 2537, 22191, 68, 6665, 5853, 5736, 16035, 8727, 19582, 18263, 4493, 3998, 15739, 3638, 11181, 16490, 6470, 8066, 11064, 19977, 10006, 13432, 11587, 19218, 12058, 19210, 812, 19016, 10703, 9600, 15389, 2607, 11720, 896, 9771, 6744, 21572, 2694, 11346, 18106, 7772, 16834, 310, 12379, 16606, 4904, 10926, 3479, 6243, 20183, 6432, 2394, 9933, 11293, 3796, 11947, 5616, 14259, 923, 14143, 8038, 19061, 17031, 11279, 1775, 12745, 21470, 11786, 16139, 2469, 21404, 9846, 13283, 17814, 2450, 8610, 11246, 10570, 15345, 3546, 5773, 12365, 7580, 12734, 6540, 7731, 10986, 5185, 18207, 3310, 1260, 9070, 10223, 12774, 17653, 9130, 6536, 18312, 7507, 18561, 13228, 2041, 13576, 20729, 1058, 18067, 2382, 5455, 5710, 21858, 1567, 17976, 12243, 16056, 9, 13334, 20460, 3185, 6943, 12602, 1304, 6542, 15820, 22132, 8188, 18415, 8611, 1729, 20617, 5987, 7110, 4541, 21819, 10167, 20643, 537, 6874, 5000, 4063, 21943, 12630, 5631, 5755, 22243, 2029, 8627, 1184, 7812, 18778, 12261, 21801, 3999, 172, 9444, 14079, 262, 22197, 17763, 15056, 18914, 20458, 20769, 15098, 11411, 3733, 8401, 1008, 8435, 8581, 8514, 2191, 17086, 18062, 4597, 15092, 21335, 17952, 16232, 12233, 3774, 2405, 18324, 10362, 10528, 10074, 7265, 16950, 18297, 3143, 9568, 9682, 15419, 12904, 13304, 5340, 11366, 19223, 6154, 1856, 13567, 9422, 13920, 3334, 18533, 5816, 13038, 9810, 1465, 14888, 18240, 22792, 17441, 14547, 17599, 14, 21263, 5350, 10342, 8550, 7734, 10645, 7069, 4992, 12647, 13936, 19047, 17780, 14349, 16684, 17475, 10107, 16110, 13020, 10137, 13262, 10633, 8104, 13428, 17289, 5974, 20136, 672, 8407, 13852, 11748, 10253, 20562, 3957, 21060, 8942, 7846, 7098, 10589, 5239, 5156, 20808, 12296, 2543, 16819, 15501, 332, 11500, 6981, 10025, 11171, 17819, 3687, 845, 19430, 19399, 14361, 13709, 4942, 8394, 20693, 16529, 13279, 2820, 21111, 10581, 4393, 7180, 3473, 20097, 6436, 20410, 13287, 17241, 14997, 11316, 6667, 2325, 18123, 17062, 18291, 6350, 18256, 14823, 11532, 5969, 3173, 8215, 20246, 12293, 18217, 457, 8411, 9651, 1720, 255, 7962, 7202, 8285, 6479, 828, 8565, 8614, 20010, 3436, 9765, 19965, 4777, 13140, 1130, 11446, 3429, 6136, 21865, 20431, 14172, 11744, 600, 15731, 21406, 5675, 2407, 16234, 18520, 3845, 2501, 16712, 11144, 5485, 8273, 20538, 6026, 22442, 20215, 4348, 11119, 345, 2628, 12452, 17317, 1837, 13571, 22473, 17531, 15165, 12066, 21883, 20834, 13744, 15065, 15728, 10879, 21884, 19385, 69, 11688, 11676, 12445, 11705, 12578, 18958, 1490, 17413, 5625, 13572, 15206, 6635, 17396, 890, 21240, 17571, 9685, 15847, 22055, 19392, 4711]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected->  18263  labels_selected->  18263  test_idxs->  4566 labels_not_selected->  4566\n",
      "Split into 18263 train and 4566 test lebels.\n",
      "embedding_DataFrame.shape->  (28, 22867)\n",
      "Number of Nodes-> 22867\n",
      "\n",
      "Random graph clustering started.\n",
      "\n",
      "Target Train Array Shape->  (6066, 1)  Target Test Array Shape->  (1515, 1)\n",
      "Target Train Array Shape->  (6032, 1)  Target Test Array Shape->  (1509, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Loss:   0%|          | 0/50 [00:00<?, ?it/s]<ipython-input-1415-42505503192d>:43: UserWarning: Using a target size (torch.Size([6166])) that is different to the input size (torch.Size([6166, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  average_loss = torch.nn.functional.mse_loss(predictions[train_nodes], target.to(torch.float32)) ##### Changed for Regression #####\n",
      "<ipython-input-1415-42505503192d>:43: UserWarning: Using a target size (torch.Size([6066])) that is different to the input size (torch.Size([6066, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  average_loss = torch.nn.functional.mse_loss(predictions[train_nodes], target.to(torch.float32)) ##### Changed for Regression #####\n",
      "<ipython-input-1415-42505503192d>:43: UserWarning: Using a target size (torch.Size([6032])) that is different to the input size (torch.Size([6032, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  average_loss = torch.nn.functional.mse_loss(predictions[train_nodes], target.to(torch.float32)) ##### Changed for Regression #####\n",
      "Train Loss: 5.84756e+13:  10%|█         | 5/50 [00:00<00:01, 39.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Train Array Shape->  (6166, 1)  Target Test Array Shape->  (1541, 1)\n",
      "feature_count_1->  28  class_count_1->  1\n",
      "Training started.\n",
      "\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a369d180>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6166, 1])  Target Train Nodes->  torch.Size([6166])\n",
      "Avg MSE Loss->  tensor(3.0470e+13, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a369d480>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6066, 1])  Target Train Nodes->  torch.Size([6066])\n",
      "Avg MSE Loss->  tensor(2.9987e+13, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a369d580>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6032, 1])  Target Train Nodes->  torch.Size([6032])\n",
      "Avg MSE Loss->  tensor(1.1577e+14, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a369da40>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6066, 1])  Target Train Nodes->  torch.Size([6066])\n",
      "Avg MSE Loss->  tensor(2.9987e+13, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a369dd00>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6166, 1])  Target Train Nodes->  torch.Size([6166])\n",
      "Avg MSE Loss->  tensor(3.0469e+13, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a369d1c0>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6032, 1])  Target Train Nodes->  torch.Size([6032])\n",
      "Avg MSE Loss->  tensor(1.1577e+14, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a369d280>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6066, 1])  Target Train Nodes->  torch.Size([6066])\n",
      "Avg MSE Loss->  tensor(2.9987e+13, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a369de00>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6032, 1])  Target Train Nodes->  torch.Size([6032])\n",
      "Avg MSE Loss->  tensor(1.1577e+14, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a369d140>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6166, 1])  Target Train Nodes->  torch.Size([6166])\n",
      "Avg MSE Loss->  tensor(3.0468e+13, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a369ddc0>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6032, 1])  Target Train Nodes->  torch.Size([6032])\n",
      "Avg MSE Loss->  tensor(1.1577e+14, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a369dec0>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6166, 1])  Target Train Nodes->  torch.Size([6166])\n",
      "Avg MSE Loss->  tensor(3.0467e+13, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a369df40>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6066, 1])  Target Train Nodes->  torch.Size([6066])\n",
      "Avg MSE Loss->  tensor(2.9985e+13, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36c4e00>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6032, 1])  Target Train Nodes->  torch.Size([6032])\n",
      "Avg MSE Loss->  tensor(1.1577e+14, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36c41c0>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6166, 1])  Target Train Nodes->  torch.Size([6166])\n",
      "Avg MSE Loss->  tensor(3.0466e+13, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36c46c0>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6066, 1])  Target Train Nodes->  torch.Size([6066])\n",
      "Avg MSE Loss->  tensor(2.9983e+13, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36c4880>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6032, 1])  Target Train Nodes->  torch.Size([6032])\n",
      "Avg MSE Loss->  tensor(1.1576e+14, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36c4300>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6166, 1])  Target Train Nodes->  torch.Size([6166])\n",
      "Avg MSE Loss->  tensor(3.0464e+13, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36c4340>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6066, 1])  Target Train Nodes->  torch.Size([6066])\n",
      "Avg MSE Loss->  tensor(2.9981e+13, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36c4280>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6032, 1])  Target Train Nodes->  torch.Size([6032])\n",
      "Avg MSE Loss->  tensor(1.1576e+14, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36c4a80>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6166, 1])  Target Train Nodes->  torch.Size([6166])\n",
      "Avg MSE Loss->  tensor(3.0461e+13, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36c4640>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6066, 1])  Target Train Nodes->  torch.Size([6066])\n",
      "Avg MSE Loss->  tensor(2.9978e+13, device='cuda:0', grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Loss: 5.84363e+13:  26%|██▌       | 13/50 [00:00<00:00, 38.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36c4d40>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6066, 1])  Target Train Nodes->  torch.Size([6066])\n",
      "Avg MSE Loss->  tensor(2.9977e+13, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36c4d40>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6166, 1])  Target Train Nodes->  torch.Size([6166])\n",
      "Avg MSE Loss->  tensor(3.0457e+13, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36c4280>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6032, 1])  Target Train Nodes->  torch.Size([6032])\n",
      "Avg MSE Loss->  tensor(1.1576e+14, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36c4280>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6166, 1])  Target Train Nodes->  torch.Size([6166])\n",
      "Avg MSE Loss->  tensor(3.0455e+13, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36c4640>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6032, 1])  Target Train Nodes->  torch.Size([6032])\n",
      "Avg MSE Loss->  tensor(1.1576e+14, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36c4240>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6066, 1])  Target Train Nodes->  torch.Size([6066])\n",
      "Avg MSE Loss->  tensor(2.9971e+13, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36c4f80>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6066, 1])  Target Train Nodes->  torch.Size([6066])\n",
      "Avg MSE Loss->  tensor(2.9970e+13, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36c4d80>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6166, 1])  Target Train Nodes->  torch.Size([6166])\n",
      "Avg MSE Loss->  tensor(3.0448e+13, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36c4900>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6032, 1])  Target Train Nodes->  torch.Size([6032])\n",
      "Avg MSE Loss->  tensor(1.1575e+14, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36c4640>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6032, 1])  Target Train Nodes->  torch.Size([6032])\n",
      "Avg MSE Loss->  tensor(1.1575e+14, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36c4740>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6066, 1])  Target Train Nodes->  torch.Size([6066])\n",
      "Avg MSE Loss->  tensor(2.9964e+13, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36c49c0>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6166, 1])  Target Train Nodes->  torch.Size([6166])\n",
      "Avg MSE Loss->  tensor(3.0440e+13, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36c4780>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6166, 1])  Target Train Nodes->  torch.Size([6166])\n",
      "Avg MSE Loss->  tensor(3.0438e+13, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36c4400>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6066, 1])  Target Train Nodes->  torch.Size([6066])\n",
      "Avg MSE Loss->  tensor(2.9958e+13, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36c4e40>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6032, 1])  Target Train Nodes->  torch.Size([6032])\n",
      "Avg MSE Loss->  tensor(1.1574e+14, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36c4cc0>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6166, 1])  Target Train Nodes->  torch.Size([6166])\n",
      "Avg MSE Loss->  tensor(3.0431e+13, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36c4bc0>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6066, 1])  Target Train Nodes->  torch.Size([6066])\n",
      "Avg MSE Loss->  tensor(2.9951e+13, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36c43c0>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6032, 1])  Target Train Nodes->  torch.Size([6032])\n",
      "Avg MSE Loss->  tensor(1.1573e+14, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36c4580>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6066, 1])  Target Train Nodes->  torch.Size([6066])\n",
      "Avg MSE Loss->  tensor(2.9947e+13, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36c4680>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6166, 1])  Target Train Nodes->  torch.Size([6166])\n",
      "Avg MSE Loss->  tensor(3.0420e+13, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36c4f00>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6032, 1])  Target Train Nodes->  torch.Size([6032])\n",
      "Avg MSE Loss->  tensor(1.1572e+14, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36d1fc0>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6066, 1])  Target Train Nodes->  torch.Size([6066])\n",
      "Avg MSE Loss->  tensor(2.9939e+13, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36d1040>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6166, 1])  Target Train Nodes->  torch.Size([6166])\n",
      "Avg MSE Loss->  tensor(3.0411e+13, device='cuda:0', grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Loss: 5.83591e+13:  42%|████▏     | 21/50 [00:00<00:00, 36.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36c4580>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6032, 1])  Target Train Nodes->  torch.Size([6032])\n",
      "Avg MSE Loss->  tensor(1.1572e+14, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36c4880>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6166, 1])  Target Train Nodes->  torch.Size([6166])\n",
      "Avg MSE Loss->  tensor(3.0404e+13, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36c4800>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6066, 1])  Target Train Nodes->  torch.Size([6066])\n",
      "Avg MSE Loss->  tensor(2.9927e+13, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36c4b40>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6032, 1])  Target Train Nodes->  torch.Size([6032])\n",
      "Avg MSE Loss->  tensor(1.1571e+14, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36c4a80>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6066, 1])  Target Train Nodes->  torch.Size([6066])\n",
      "Avg MSE Loss->  tensor(2.9921e+13, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36c4d40>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6032, 1])  Target Train Nodes->  torch.Size([6032])\n",
      "Avg MSE Loss->  tensor(1.1570e+14, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36c4700>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6166, 1])  Target Train Nodes->  torch.Size([6166])\n",
      "Avg MSE Loss->  tensor(3.0386e+13, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36c4cc0>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6032, 1])  Target Train Nodes->  torch.Size([6032])\n",
      "Avg MSE Loss->  tensor(1.1569e+14, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36c4f40>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6066, 1])  Target Train Nodes->  torch.Size([6066])\n",
      "Avg MSE Loss->  tensor(2.9907e+13, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36c48c0>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6166, 1])  Target Train Nodes->  torch.Size([6166])\n",
      "Avg MSE Loss->  tensor(3.0373e+13, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36c4a00>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6166, 1])  Target Train Nodes->  torch.Size([6166])\n",
      "Avg MSE Loss->  tensor(3.0369e+13, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36c4080>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6066, 1])  Target Train Nodes->  torch.Size([6066])\n",
      "Avg MSE Loss->  tensor(2.9896e+13, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36d1140>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6032, 1])  Target Train Nodes->  torch.Size([6032])\n",
      "Avg MSE Loss->  tensor(1.1568e+14, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36d1040>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6166, 1])  Target Train Nodes->  torch.Size([6166])\n",
      "Avg MSE Loss->  tensor(3.0355e+13, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36d1ac0>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6032, 1])  Target Train Nodes->  torch.Size([6032])\n",
      "Avg MSE Loss->  tensor(1.1567e+14, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36d1b80>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6066, 1])  Target Train Nodes->  torch.Size([6066])\n",
      "Avg MSE Loss->  tensor(2.9879e+13, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36d1140>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6166, 1])  Target Train Nodes->  torch.Size([6166])\n",
      "Avg MSE Loss->  tensor(3.0340e+13, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36d1580>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6032, 1])  Target Train Nodes->  torch.Size([6032])\n",
      "Avg MSE Loss->  tensor(1.1565e+14, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36d1980>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6066, 1])  Target Train Nodes->  torch.Size([6066])\n",
      "Avg MSE Loss->  tensor(2.9866e+13, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36d1100>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6066, 1])  Target Train Nodes->  torch.Size([6066])\n",
      "Avg MSE Loss->  tensor(2.9861e+13, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36d1780>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6032, 1])  Target Train Nodes->  torch.Size([6032])\n",
      "Avg MSE Loss->  tensor(1.1564e+14, device='cuda:0', grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Loss: 5.82356e+13:  50%|█████     | 25/50 [00:00<00:00, 36.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36c4a80>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6166, 1])  Target Train Nodes->  torch.Size([6166])\n",
      "Avg MSE Loss->  tensor(3.0313e+13, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36c4180>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6066, 1])  Target Train Nodes->  torch.Size([6066])\n",
      "Avg MSE Loss->  tensor(2.9846e+13, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36c48c0>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6166, 1])  Target Train Nodes->  torch.Size([6166])\n",
      "Avg MSE Loss->  tensor(3.0302e+13, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36c4700>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6032, 1])  Target Train Nodes->  torch.Size([6032])\n",
      "Avg MSE Loss->  tensor(1.1562e+14, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36d1c80>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6032, 1])  Target Train Nodes->  torch.Size([6032])\n",
      "Avg MSE Loss->  tensor(1.1562e+14, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36d1580>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6166, 1])  Target Train Nodes->  torch.Size([6166])\n",
      "Avg MSE Loss->  tensor(3.0283e+13, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36d1780>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6066, 1])  Target Train Nodes->  torch.Size([6066])\n",
      "Avg MSE Loss->  tensor(2.9820e+13, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36d1780>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6166, 1])  Target Train Nodes->  torch.Size([6166])\n",
      "Avg MSE Loss->  tensor(3.0271e+13, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36d1780>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6066, 1])  Target Train Nodes->  torch.Size([6066])\n",
      "Avg MSE Loss->  tensor(2.9808e+13, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36d1640>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6032, 1])  Target Train Nodes->  torch.Size([6032])\n",
      "Avg MSE Loss->  tensor(1.1559e+14, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36d1080>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6032, 1])  Target Train Nodes->  torch.Size([6032])\n",
      "Avg MSE Loss->  tensor(1.1558e+14, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36d17c0>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6066, 1])  Target Train Nodes->  torch.Size([6066])\n",
      "Avg MSE Loss->  tensor(2.9791e+13, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36d1ac0>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6166, 1])  Target Train Nodes->  torch.Size([6166])\n",
      "Avg MSE Loss->  tensor(3.0237e+13, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36d1980>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6032, 1])  Target Train Nodes->  torch.Size([6032])\n",
      "Avg MSE Loss->  tensor(1.1556e+14, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36d1580>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6166, 1])  Target Train Nodes->  torch.Size([6166])\n",
      "Avg MSE Loss->  tensor(3.0222e+13, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36d18c0>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6066, 1])  Target Train Nodes->  torch.Size([6066])\n",
      "Avg MSE Loss->  tensor(2.9766e+13, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36d1c80>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6066, 1])  Target Train Nodes->  torch.Size([6066])\n",
      "Avg MSE Loss->  tensor(2.9759e+13, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36d1500>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6032, 1])  Target Train Nodes->  torch.Size([6032])\n",
      "Avg MSE Loss->  tensor(1.1554e+14, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36d1480>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6166, 1])  Target Train Nodes->  torch.Size([6166])\n",
      "Avg MSE Loss->  tensor(3.0192e+13, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36d1f80>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6166, 1])  Target Train Nodes->  torch.Size([6166])\n",
      "Avg MSE Loss->  tensor(3.0184e+13, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36d1fc0>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6032, 1])  Target Train Nodes->  torch.Size([6032])\n",
      "Avg MSE Loss->  tensor(1.1552e+14, device='cuda:0', grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Loss: 5.8068e+13:  66%|██████▌   | 33/50 [00:00<00:00, 35.42it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36d1640>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6066, 1])  Target Train Nodes->  torch.Size([6066])\n",
      "Avg MSE Loss->  tensor(2.9725e+13, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36d1980>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6032, 1])  Target Train Nodes->  torch.Size([6032])\n",
      "Avg MSE Loss->  tensor(1.1550e+14, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36d1500>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6166, 1])  Target Train Nodes->  torch.Size([6166])\n",
      "Avg MSE Loss->  tensor(3.0152e+13, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36d1c80>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6066, 1])  Target Train Nodes->  torch.Size([6066])\n",
      "Avg MSE Loss->  tensor(2.9704e+13, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36d1340>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6032, 1])  Target Train Nodes->  torch.Size([6032])\n",
      "Avg MSE Loss->  tensor(1.1548e+14, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36d1780>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6166, 1])  Target Train Nodes->  torch.Size([6166])\n",
      "Avg MSE Loss->  tensor(3.0127e+13, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36d1780>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6066, 1])  Target Train Nodes->  torch.Size([6066])\n",
      "Avg MSE Loss->  tensor(2.9681e+13, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36d18c0>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6066, 1])  Target Train Nodes->  torch.Size([6066])\n",
      "Avg MSE Loss->  tensor(2.9674e+13, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36d1580>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6032, 1])  Target Train Nodes->  torch.Size([6032])\n",
      "Avg MSE Loss->  tensor(1.1545e+14, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36d1980>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6166, 1])  Target Train Nodes->  torch.Size([6166])\n",
      "Avg MSE Loss->  tensor(3.0091e+13, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36d17c0>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6066, 1])  Target Train Nodes->  torch.Size([6066])\n",
      "Avg MSE Loss->  tensor(2.9650e+13, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36d1f80>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6032, 1])  Target Train Nodes->  torch.Size([6032])\n",
      "Avg MSE Loss->  tensor(1.1543e+14, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36d1500>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6166, 1])  Target Train Nodes->  torch.Size([6166])\n",
      "Avg MSE Loss->  tensor(3.0064e+13, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36d1ac0>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6166, 1])  Target Train Nodes->  torch.Size([6166])\n",
      "Avg MSE Loss->  tensor(3.0054e+13, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36d1640>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6066, 1])  Target Train Nodes->  torch.Size([6066])\n",
      "Avg MSE Loss->  tensor(2.9617e+13, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36d1800>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6032, 1])  Target Train Nodes->  torch.Size([6032])\n",
      "Avg MSE Loss->  tensor(1.1540e+14, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36d16c0>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6166, 1])  Target Train Nodes->  torch.Size([6166])\n",
      "Avg MSE Loss->  tensor(3.0025e+13, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36d1340>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6066, 1])  Target Train Nodes->  torch.Size([6066])\n",
      "Avg MSE Loss->  tensor(2.9592e+13, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36d1dc0>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6032, 1])  Target Train Nodes->  torch.Size([6032])\n",
      "Avg MSE Loss->  tensor(1.1537e+14, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36d1d00>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6166, 1])  Target Train Nodes->  torch.Size([6166])\n",
      "Avg MSE Loss->  tensor(2.9995e+13, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36d1c80>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6066, 1])  Target Train Nodes->  torch.Size([6066])\n",
      "Avg MSE Loss->  tensor(2.9565e+13, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Loss: 5.78575e+13:  82%|████████▏ | 41/50 [00:01<00:00, 35.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36c4680>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6032, 1])  Target Train Nodes->  torch.Size([6032])\n",
      "Avg MSE Loss->  tensor(1.1534e+14, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a369d780>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6166, 1])  Target Train Nodes->  torch.Size([6166])\n",
      "Avg MSE Loss->  tensor(2.9964e+13, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36c4880>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6066, 1])  Target Train Nodes->  torch.Size([6066])\n",
      "Avg MSE Loss->  tensor(2.9538e+13, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36c4a80>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6032, 1])  Target Train Nodes->  torch.Size([6032])\n",
      "Avg MSE Loss->  tensor(1.1532e+14, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36d1fc0>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6166, 1])  Target Train Nodes->  torch.Size([6166])\n",
      "Avg MSE Loss->  tensor(2.9933e+13, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36d1080>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6066, 1])  Target Train Nodes->  torch.Size([6066])\n",
      "Avg MSE Loss->  tensor(2.9510e+13, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36d1800>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6032, 1])  Target Train Nodes->  torch.Size([6032])\n",
      "Avg MSE Loss->  tensor(1.1529e+14, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36d1340>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6166, 1])  Target Train Nodes->  torch.Size([6166])\n",
      "Avg MSE Loss->  tensor(2.9900e+13, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36d1d00>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6032, 1])  Target Train Nodes->  torch.Size([6032])\n",
      "Avg MSE Loss->  tensor(1.1527e+14, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36d1d00>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6066, 1])  Target Train Nodes->  torch.Size([6066])\n",
      "Avg MSE Loss->  tensor(2.9472e+13, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36d1800>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6066, 1])  Target Train Nodes->  torch.Size([6066])\n",
      "Avg MSE Loss->  tensor(2.9462e+13, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36d1d00>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6032, 1])  Target Train Nodes->  torch.Size([6032])\n",
      "Avg MSE Loss->  tensor(1.1524e+14, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36d1280>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6166, 1])  Target Train Nodes->  torch.Size([6166])\n",
      "Avg MSE Loss->  tensor(2.9844e+13, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36d1480>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6032, 1])  Target Train Nodes->  torch.Size([6032])\n",
      "Avg MSE Loss->  tensor(1.1522e+14, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36d13c0>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6066, 1])  Target Train Nodes->  torch.Size([6066])\n",
      "Avg MSE Loss->  tensor(2.9422e+13, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36d1200>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6166, 1])  Target Train Nodes->  torch.Size([6166])\n",
      "Avg MSE Loss->  tensor(2.9810e+13, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36d18c0>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6066, 1])  Target Train Nodes->  torch.Size([6066])\n",
      "Avg MSE Loss->  tensor(2.9402e+13, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36d1ec0>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6032, 1])  Target Train Nodes->  torch.Size([6032])\n",
      "Avg MSE Loss->  tensor(1.1518e+14, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36d1280>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6166, 1])  Target Train Nodes->  torch.Size([6166])\n",
      "Avg MSE Loss->  tensor(2.9775e+13, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36d17c0>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6166, 1])  Target Train Nodes->  torch.Size([6166])\n",
      "Avg MSE Loss->  tensor(2.9763e+13, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36d1240>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6032, 1])  Target Train Nodes->  torch.Size([6032])\n",
      "Avg MSE Loss->  tensor(1.1515e+14, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36d1d00>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6066, 1])  Target Train Nodes->  torch.Size([6066])\n",
      "Avg MSE Loss->  tensor(2.9349e+13, device='cuda:0', grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Loss: 5.75757e+13: 100%|██████████| 50/50 [00:01<00:00, 36.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36c4a00>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6066, 1])  Target Train Nodes->  torch.Size([6066])\n",
      "Avg MSE Loss->  tensor(2.9339e+13, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a369df00>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6032, 1])  Target Train Nodes->  torch.Size([6032])\n",
      "Avg MSE Loss->  tensor(1.1512e+14, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36c4940>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6166, 1])  Target Train Nodes->  torch.Size([6166])\n",
      "Avg MSE Loss->  tensor(2.9702e+13, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36d1280>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6066, 1])  Target Train Nodes->  torch.Size([6066])\n",
      "Avg MSE Loss->  tensor(2.9306e+13, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36d1280>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6032, 1])  Target Train Nodes->  torch.Size([6032])\n",
      "Avg MSE Loss->  tensor(1.1508e+14, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36d1d00>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6166, 1])  Target Train Nodes->  torch.Size([6166])\n",
      "Avg MSE Loss->  tensor(2.9664e+13, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36d1d00>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6032, 1])  Target Train Nodes->  torch.Size([6032])\n",
      "Avg MSE Loss->  tensor(1.1506e+14, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36d1d00>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6166, 1])  Target Train Nodes->  torch.Size([6166])\n",
      "Avg MSE Loss->  tensor(2.9639e+13, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36d1080>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6066, 1])  Target Train Nodes->  torch.Size([6066])\n",
      "Avg MSE Loss->  tensor(2.9251e+13, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36d1280>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6032, 1])  Target Train Nodes->  torch.Size([6032])\n",
      "Avg MSE Loss->  tensor(1.1503e+14, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36d18c0>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6066, 1])  Target Train Nodes->  torch.Size([6066])\n",
      "Avg MSE Loss->  tensor(2.9228e+13, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36d1500>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6166, 1])  Target Train Nodes->  torch.Size([6166])\n",
      "Avg MSE Loss->  tensor(2.9587e+13, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36d1980>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6032, 1])  Target Train Nodes->  torch.Size([6032])\n",
      "Avg MSE Loss->  tensor(1.1499e+14, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36d19c0>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6166, 1])  Target Train Nodes->  torch.Size([6166])\n",
      "Avg MSE Loss->  tensor(2.9561e+13, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36d1bc0>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6066, 1])  Target Train Nodes->  torch.Size([6066])\n",
      "Avg MSE Loss->  tensor(2.9181e+13, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36d1240>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6032, 1])  Target Train Nodes->  torch.Size([6032])\n",
      "Avg MSE Loss->  tensor(1.1496e+14, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36d18c0>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6166, 1])  Target Train Nodes->  torch.Size([6166])\n",
      "Avg MSE Loss->  tensor(2.9521e+13, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36d1f00>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6066, 1])  Target Train Nodes->  torch.Size([6066])\n",
      "Avg MSE Loss->  tensor(2.9146e+13, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36d1480>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6032, 1])  Target Train Nodes->  torch.Size([6032])\n",
      "Avg MSE Loss->  tensor(1.1492e+14, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36d1540>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6166, 1])  Target Train Nodes->  torch.Size([6166])\n",
      "Avg MSE Loss->  tensor(2.9480e+13, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "Know the Type->  <class 'torch.Tensor'> 2\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36d1340>\n",
      "*** Check Carefully *** Input Train Nodes->  torch.Size([6066, 1])  Target Train Nodes->  torch.Size([6066])\n",
      "Avg MSE Loss->  tensor(2.9110e+13, device='cuda:0', grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd7a36d18c0>\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd804d6a600>\n",
      "StackedGCN Forward pred size->  <built-in method size of Tensor object at 0x7fd804d6ae40>\n",
      "4565  Type <class 'numpy.ndarray'>  ********   Type  <class 'numpy.ndarray'> 4565\n",
      "R Squred->  -0.16026211020793557\n",
      "Regression->               Actual  Predicted\n",
      "Actual     1.000000   0.140048\n",
      "Predicted  0.140048   1.000000\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Parsing command line parameters, reading data, graph decomposition, fitting a ClusterGCN and scoring the model.\n",
    "    \"\"\"\n",
    "    args = parameter_processor()\n",
    "    nx_graph, target, train_Target, test_Target, features = read_HPC_data(args)\n",
    "    print(\"Number of Nodes->\", len(list(nx_graph.nodes())))\n",
    "    clustering_machine = ClusteringMachine(args, nx_graph, features, target, train_Target, test_Target)\n",
    "    clustering_machine.decompose()\n",
    "    gcn_trainer = ClusterGCNTrainer(args, clustering_machine)\n",
    "    gcn_trainer.train()\n",
    "    gcn_trainer.test()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average = temp.groupby('owner', as_index=False)['cpu','maxvmem','reqTime','reqMem','mem'].mean()\n",
    "# average.columns = ['owner','aCPU','aMaxvmem','aReqtime','aReqmem','aMem']\n",
    "# t3 = pd.merge(temp, average, on=['owner'])\n",
    "# t3.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[['owner', 'failed', 'project', 'cpu', 'maxvmem', 'reqTime', 'reqMem', 'people','mem']]\n",
    "# 'CPUtimeraw' and 'MaxRSS' should be the predicted value, TimelimitRaw is not there\n",
    "# cpu -> (AveCPU/TotalCPU/) CPUtimeraw, maxvmem -> MaxVMSize, reqTime-> TimelimitRaw, ReqMem-> ReqMem, mem -> MaxRSS (Maximum resident set size)\n",
    "\n",
    "# average = temp.groupby('owner', as_index=False)['cpu','maxvmem','reqTime','ReqMem','mem'].mean()\n",
    "# average.columns = ['owner','aCPU','aMaxvmem','aReqtime','aReqmem','aMem']\n",
    "# t3 = pd.merge(temp, average, on=['owner'])\n",
    "# t3.columns\n",
    "\n",
    "# nx_G=nx.read_edgelist('/home/abose/HPC Analytics GCN/src_des_edges.csv', delimiter=',', nodetype=str)\n",
    "# H = nx.convert_node_labels_to_integers(nx_G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-488-2caf5216d96f>:9: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  torch.nn.functional.nll_loss(torch.nn.functional.log_softmax(input1), target)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(2.4814, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 488,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 5\n",
    "num_classes = 8\n",
    "x = torch.rand(batch_size, num_classes)\n",
    "y = torch.randint(num_classes, (batch_size,))\n",
    "\n",
    "input1 = torch.randn(3, 15, requires_grad=True)\n",
    "# each element in target has to have 0 <= value < C\n",
    "target = torch.tensor([1, 0, 4])\n",
    "torch.nn.functional.nll_loss(torch.nn.functional.log_softmax(input1), target)\n",
    "# output\n",
    "# output.backward()\n",
    "# torch.nn.functional.binary_cross_entropy(input1, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'NewJobID'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-489-68d3996bd3f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mconcat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mslurm_Cleaned_Demo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'DataFrame Column'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslurm_Cleaned_Demo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'MaxVMSize'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mgroup_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslurm_Cleaned_Demo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'NewJobID'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;31m# group_df = group_df.agg({'NodeList':name_join, 'MaxVMSize': 'max', 'MaxRSS': 'max', 'AveVMSize': 'max', 'AveRSS':'max', 'AssocID': 'max', 'ReqCPUS':'max', 'UID':'max', 'role':'first','university':'first' ,'GID':'max', 'q5':'max', 'q6':'max', 'q7':'max'})\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mgroup_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'NodeList'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mname_join\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'MaxVMSize'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'max'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pyTorch_env/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mgroupby\u001b[0;34m(self, by, axis, level, as_index, sort, group_keys, squeeze, observed, dropna)\u001b[0m\n\u001b[1;32m   6508\u001b[0m         \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis_number\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6509\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6510\u001b[0;31m         return DataFrameGroupBy(\n\u001b[0m\u001b[1;32m   6511\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6512\u001b[0m             \u001b[0mkeys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pyTorch_env/lib/python3.8/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, squeeze, observed, mutated, dropna)\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrouper\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_grouper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m             grouper, exclusions, obj = get_grouper(\n\u001b[0m\u001b[1;32m    526\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m                 \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pyTorch_env/lib/python3.8/site-packages/pandas/core/groupby/grouper.py\u001b[0m in \u001b[0;36mget_grouper\u001b[0;34m(obj, key, axis, level, sort, observed, mutated, validate, dropna)\u001b[0m\n\u001b[1;32m    779\u001b[0m                 \u001b[0min_axis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 781\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    782\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGrouper\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mgpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    783\u001b[0m             \u001b[0;31m# Add key to exclusions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'NewJobID'"
     ]
    }
   ],
   "source": [
    "# df = pd.DataFrame([[60, 64, np.nan], ['60.ext', 67, 100], ['60.eng', 72, 0], [72, 75, np.nan], ['72.ext', 79, 101], ['72.eng', 82, 0], [82, 85, np.nan]], columns=['JobID', 'UID', 'MaxRSS'])\n",
    "# .apply(lambda x: (\" \".join(x[\"t\"])).lower())\n",
    "# slurm_Cleaned_Demo.groupby(slurm_Cleaned_Demo['NewJobID']).agg({'JobID': 'first', 'MaxVMSize': 'max', 'MaxRSS': 'max', 'AveVMSize': 'max', 'AveRSS':'max', 'AssocID': 'max', 'ReqCPUS':'max', 'UID':'max', 'GID':'max', 'q5':'max', 'q6':'max', 'q7':'max'}).reset_index()\n",
    "# slurm_Cleaned_Demo.groupby(slurm_Cleaned_Demo['NewJobID']).agg({'JobID': 'first', 'UID': 'max', 'NTasks':'max'}).reset_index()\n",
    "def name_join(list_names, concat='-'):\n",
    "    return concat.join(list_names)\n",
    "slurm_Cleaned_Demo['DataFrame Column'] = slurm_Cleaned_Demo['MaxVMSize'].astype(float)\n",
    "group_df = slurm_Cleaned_Demo.groupby('NewJobID')\n",
    "# group_df = group_df.agg({'NodeList':name_join, 'MaxVMSize': 'max', 'MaxRSS': 'max', 'AveVMSize': 'max', 'AveRSS':'max', 'AssocID': 'max', 'ReqCPUS':'max', 'UID':'max', 'role':'first','university':'first' ,'GID':'max', 'q5':'max', 'q6':'max', 'q7':'max'})\n",
    "group_df = group_df.agg({'NodeList':name_join, 'MaxVMSize': 'max'})\n",
    "s = group_df['NewJobID'].str.split('-').apply(pd.Series, 1).stack()\n",
    "s.index = s.index.droplevel(-1)\n",
    "s.name = 'NewJobID'\n",
    "del group_df['NewJobID']\n",
    "group_df = group_df.join(s)\n",
    "group_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_scatter import scatter_mean\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "dataset = TUDataset(root='/tmp/ENZYMES', name='ENZYMES', use_node_attr=True)\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "for data in loader:\n",
    "    print(data, len(data.to_data_list()), data.y.unique(), data.to_data_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = torch.nn.LogSoftmax(dim=1)\n",
    "loss = torch.nn.NLLLoss()\n",
    "# input is of size N x C = 3 x 5\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "# each element in target has to have 0 <= value < C\n",
    "target = torch.tensor([1, 0, 4])\n",
    "output = loss(m(input), target)\n",
    "output.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7709, -1.0619,  0.4434,  1.3743, -0.3757],\n",
       "        [-0.1636,  0.0159, -0.1985,  0.1198, -0.5375],\n",
       "        [ 0.2817, -0.0839,  0.6825,  0.9339,  0.6968]], requires_grad=True)"
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
